{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7DHn1DSxC8PB"
   },
   "source": [
    "# RNN\n",
    "\n",
    "-입력과 출력을 시퀀스로 처리하는 모델\n",
    "-은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징을 갖고있음\n",
    "\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ou7UJi3gC8PB"
   },
   "source": [
    "  일 대 다 : 하나의 입력에 대해서 여러개의 출력(one-to-many)\n",
    "• 하나의 사진 이미지 입력에 대해서 사진의 제목을 출력을 내놓는 이미지 캡셔닝(Image\n",
    "Captioning) 작업에 사용\n",
    "• 사진의 제목은 단어들의 나열이므로 시퀀스의 형태를 가짐\n",
    " 다 대 일 : 다수의 입력에 대해서 하나의 출력(many-to-one)\n",
    "• 입력 데이터으로부터 긍정적 감성인지 부정적 감성인지를 판별하는 감성 분류\n",
    "(Sentiment Classification),\n",
    "• 입력 데이터가 어떤 종류의 문서인지를 판별하는 문서 분류(Document Classification)에\n",
    "사용\n",
    " 다 대 다(many-to-many)\n",
    "• 입력 문장으로 부터 대답을 출력하는 챗봇, 입력 문장으로부터 번역된 문장을 출력하\n",
    "는 번역기, 개체명 인식, 품사 태깅 등에 이용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ijpxXGG5C8PC"
   },
   "source": [
    "# RNN 실습\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XQSgEa84C8PC"
   },
   "source": [
    "hidden_size = 은닉 상태의 크기를 정의. 메모리 셀이 다음 시점의 메모리 셀과 출력층으로 보내는 값의 크기(output_dim)와도 동일. RNN의 용량(capacity)을 늘린다고 보면 되며, 중소형 모델의 경우 보통 128, 256, 512, 1024 등의 값을 가진다.\n",
    "timesteps = 입력 시퀀스의 길이(input_length)라고 표현하기도 함. 시점의 수.\n",
    "input_dim = 입력의 크기.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2270,
     "status": "ok",
     "timestamp": 1566355225034,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "TGquEEDIC8PD",
    "outputId": "3c669ab7-37d8-4197-a91d-18f7ed005c9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0821 02:40:24.065211 140576597014400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0821 02:40:24.104816 140576597014400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0821 02:40:24.114213 140576597014400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, input_shape=(2,10)))\n",
    "# model.add(SimpleRNN(3, input_length=2, input_dim=10))와 동일함.\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4PHh59SdC8PG"
   },
   "source": [
    "출력값이 (batch_size, output_dim) 크기의 2D 텐서일 때, output_dim은 hidden_size의 값인 3입니다. 이 경우 batch_size를 현 단계에서는 알 수 없으므로 (None, 3)이 됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2560,
     "status": "ok",
     "timestamp": 1566355225327,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "LJKjOdq0C8PG",
    "outputId": "ae465bfb-ce8e-4023-b6c3-c48b0900b6b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (8, 3)                    42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, batch_input_shape=(8,2,10)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ggYfQJkJC8PI"
   },
   "source": [
    "batch_size를 8로 기재하자, 출력의 크기가 (8, 3)이 된 것을 볼 수 있습니다. 이제 return_sequences 매개 변수에 True를 기재하여 출력값으로 (batch_size, timesteps, output_dim) 크기의 3D 텐서를 리턴하도록 모델을 만들어 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2558,
     "status": "ok",
     "timestamp": 1566355225327,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "y6roWNJ4C8PI",
    "outputId": "d7a86ff3-e1d3-45ba-96ef-8856498112c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_3 (SimpleRNN)     (8, 2, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, batch_input_shape=(8,2,10), return_sequences=True))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwSjGUMkC8PK"
   },
   "source": [
    "# RNN 이용한 언어 모델링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lxLDfvWuC8PK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 879,
     "status": "ok",
     "timestamp": 1566355355643,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "oRRIiN2ZC8PM",
    "outputId": "5a5f1a8b-ba5d-403d-9f68-e77cb7586f3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5)\n"
     ]
    }
   ],
   "source": [
    "train_X=[[0.1,4.2,1.5,1.1,2.8],[1.0,3.1,2.5,0.7,1.1],[\n",
    "    .3,2.1,1.5,2.1,0.1],[2.2,1.4,0.5,0.9,1.1]]\n",
    "\n",
    "print(np.shape(train_X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zt0fJUMQC8PO"
   },
   "source": [
    " 영웅은', '죽지', '않아요', '너만'이라는 단어들을 각 시점의 훈련 데이터인 X로\n",
    "사용\n",
    " 여기서는 RNN의 입력으로 임베딩 벡터(embedding vector)를 사용\n",
    "이는 2차원 텐서에 해당되며 (input_length, input_dim)의 크기에 해당됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1215,
     "status": "ok",
     "timestamp": 1566355355985,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "4ZruakfPC8PP",
    "outputId": "e4475e79-69be-45cd-8e91-ebdf8baebfab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 5)\n"
     ]
    }
   ],
   "source": [
    "train_X=[[[0.1,4.2,1.5,1.1,2.8],[1.0,3.1,2.5,0.7,1.1],[\n",
    "    .3,2.1,1.5,2.1,0.1],[2.2,1.4,0.5,0.9,1.1]]]\n",
    "\n",
    "train_X=np.array(train_X,dtype=np.float32)\n",
    "\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UQG2YDWmC8PQ"
   },
   "source": [
    "이를 RNN에 넣어서 RNN이 어떤 결과를 리턴하는지 확인\n",
    " RNN은 2D 텐서가 아니라 3D 텐서를 입력 받기 때문에\n",
    "위에서 만든 2D 텐서를 3D 텐서로 변경\n",
    " (batch_size, timesteps, input_dim)에 해당되는 (1, 4, 5)의 크기를 가지는 3D 텐서\n",
    "생성\n",
    " batch_size는 한 번에 RNN이 학습하는 데이터의 양을 설정하는데,\n",
    "여기서는 사실 학습하는 데이터 즉, 문장이 1개 밖에 없으므로\n",
    "batch_size는 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1529,
     "status": "ok",
     "timestamp": 1566355356303,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "UyGR8ZRzC8PU",
    "outputId": "74e99f69-1904-4bb5-b84a-207b046f61ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X :[[[0.1 4.2 1.5 1.1 2.8]\n",
      "  [1.  3.1 2.5 0.7 1.1]\n",
      "  [0.3 2.1 1.5 2.1 0.1]\n",
      "  [2.2 1.4 0.5 0.9 1.1]]], shape: (1, 4, 5)\n",
      "hidden states : [[[-0.91957676 -0.9997508  -0.05448371]\n",
      "  [-0.42947188 -0.99982375  0.9265283 ]\n",
      "  [-0.93827075 -0.90613776  0.38662696]\n",
      "  [-0.87534666 -0.9901501   0.49548334]]] , shape: (1, 4, 3)\n",
      "last hidden state : [[-0.87534666 -0.9901501   0.49548334]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "rnn=SimpleRNN(3,return_sequences=True,return_state=True)\n",
    "\n",
    "hidden_states,last_states=rnn(train_X)\n",
    "\n",
    "print('train_X :{}, shape: {}'.format(train_X,train_X.shape))   #입력으로 사용한 훈련 데이터\n",
    "\n",
    "print('hidden states : {} , shape: {}'.format(hidden_states,hidden_states.shape))   #모든 time-step의 은닉상태\n",
    "\n",
    "print('last hidden state : {}, shape: {}'.format(last_states,last_states.shape))   #마지막 은닉 상태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x3RF0vlTC8PX"
   },
   "source": [
    "출력층에 대한 설계\n",
    "\n",
    "앞서 은닉층의 RNN 셀이 각각의 시점에 대해서 은닉 상태 벡터의 크기\n",
    "즉, output_dim이 3인 벡터를 만들어내는 것을 확인\n",
    "• 그런데 실제값 y인 원-핫 벡터는 차원이 5이므로, 예측값 또한 출력층을 통해\n",
    "output_dim을 다시 5로 바꿔줄 필요가 있음. 그래야 손실 함수를 적용할 수 있기 때문\n",
    "• 이는 출력층에 model.add(Dense(5))를 통해 5개의 뉴런을 추가하면 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9CWuOwPdC8PX"
   },
   "source": [
    "다음 단어 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZrTD8RxmC8PY"
   },
   "outputs": [],
   "source": [
    "from keras_preprocessing.text import Tokenizer\n",
    "\n",
    "text=\"나랑 점심 먹으로 갈래 메뉴는 햄버거 점심 메뉴 좋지\"\n",
    "\n",
    "t=Tokenizer()\n",
    "t.fit_on_texts([text])\n",
    "encoded=t.texts_to_sequences([text])[0]\n",
    "\n",
    "#[0]을 해주지 않으면 [[contents]]와 같은 리스트 안의 리스트 형태로 저장 됨\n",
    "     # 해주면  [contents] 와 같은 리스트로 저장\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1516,
     "status": "ok",
     "timestamp": 1566355356304,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "lUAmednoC8Pa",
    "outputId": "798efa51-3e3a-49c6-82d3-d5936be596b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'점심': 1, '나랑': 2, '먹으로': 3, '갈래': 4, '메뉴는': 5, '햄버거': 6, '메뉴': 7, '좋지': 8}\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(t.word_index)+1\n",
    "#케라스 토크나이저의 정수 인코딩은 인데스가 1부터 시작하지만 원핫 인코딩에서 배열의 인덱스가 0부터시작하기 때문에 배열의 크기를 실제 단어 집합의 크기보다 +1로 생성\n",
    "\n",
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3Jtz-nzC8Pb"
   },
   "source": [
    "문장에서 두 개 단어씩 묶기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1511,
     "status": "ok",
     "timestamp": 1566355356305,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "C5gI45GhC8Pc",
    "outputId": "c6b03a4a-688f-46f7-a5e0-d752dff8cdba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 묶음의 개수 : 8 \n"
     ]
    }
   ],
   "source": [
    "sequences=list()\n",
    "for c in range(1,len(encoded)):\n",
    "    sequence=encoded[c-1 : c+1]   #단어를 두개씩 묶어서 저장 이는 x,의 관계                                      구성하기위해\n",
    "    sequences.append(sequence)\n",
    "    \n",
    "print('단어 묶음의 개수 : %d '%len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1505,
     "status": "ok",
     "timestamp": 1566355356305,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "5sfYoMHmC8Pd",
    "outputId": "795a1474-895d-41d7-f636-bcdf8b398325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 1], [1, 3], [3, 4], [4, 5], [5, 6], [6, 1], [1, 7], [7, 8]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UniP3qtXC8Pf"
   },
   "source": [
    "해당 단어의 묶음에서 첫번째 열을 X, 두번째 열을 y로 저장한다면,\n",
    "X를 현재 등장한 단어, y를 다음에 등장할 단어로 하여\n",
    "훈련 데이터로 사용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9hzxXgPCC8Pf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X,y=zip(*sequences) #첫 번째 열이 X, 두번째 열이 y\n",
    "\n",
    "X=np.array(X)\n",
    "y=np.array(y)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDdXejjzC8Pg"
   },
   "source": [
    "y 에 대해 원핫 인코딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1566355510192,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "70_deUIIC8Ph",
    "outputId": "1c5dd5cb-2c53-47de-fe02-f5f1dcc4523b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.utils import to_categorical\n",
    "y=to_categorical(y,num_classes=vocab_size)  #원핫 인코딩\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 736,
     "status": "ok",
     "timestamp": 1566355546131,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "_MJ4D5G2C8Pi",
    "outputId": "447a86f3-0b3d-4a71-a4c0-c31c93e6ca3a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0821 02:45:45.114322 139779097495424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0821 02:45:45.117377 139779097495424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0821 02:45:45.122581 139779097495424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Dense, SimpleRNN\n",
    "from keras.models import Sequential\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,9,input_length=1))\n",
    "#단어 집합의 크기는9 임베딩 벡터의 크기는 9 각 샘플 길이는 단어 한 개이므로 길이는1\n",
    "\n",
    "model.add(SimpleRNN(9))\n",
    "#RNN의 결과값으로 나오는 벡터의 차원 9로 함\n",
    "\n",
    "model.add(Dense(vocab_size,activation='softmax'))\n",
    "#출력층을 지나서 나오는 벡터의 크기도 9로 함\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3754,
     "status": "ok",
     "timestamp": 1566355642722,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "XfM59PbcC8Pk",
    "outputId": "f117aea6-5fb3-4ac1-dda9-f32b882fb769"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0821 02:47:18.696825 139779097495424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0821 02:47:18.727393 139779097495424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0821 02:47:19.001504 139779097495424 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0821 02:47:19.195265 139779097495424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - 1s - loss: 2.1934 - acc: 0.1250\n",
      "Epoch 2/500\n",
      " - 0s - loss: 2.1905 - acc: 0.1250\n",
      "Epoch 3/500\n",
      " - 0s - loss: 2.1873 - acc: 0.1250\n",
      "Epoch 4/500\n",
      " - 0s - loss: 2.1838 - acc: 0.1250\n",
      "Epoch 5/500\n",
      " - 0s - loss: 2.1803 - acc: 0.1250\n",
      "Epoch 6/500\n",
      " - 0s - loss: 2.1767 - acc: 0.1250\n",
      "Epoch 7/500\n",
      " - 0s - loss: 2.1730 - acc: 0.2500\n",
      "Epoch 8/500\n",
      " - 0s - loss: 2.1694 - acc: 0.3750\n",
      "Epoch 9/500\n",
      " - 0s - loss: 2.1656 - acc: 0.3750\n",
      "Epoch 10/500\n",
      " - 0s - loss: 2.1619 - acc: 0.3750\n",
      "Epoch 11/500\n",
      " - 0s - loss: 2.1581 - acc: 0.5000\n",
      "Epoch 12/500\n",
      " - 0s - loss: 2.1544 - acc: 0.5000\n",
      "Epoch 13/500\n",
      " - 0s - loss: 2.1506 - acc: 0.5000\n",
      "Epoch 14/500\n",
      " - 0s - loss: 2.1468 - acc: 0.6250\n",
      "Epoch 15/500\n",
      " - 0s - loss: 2.1430 - acc: 0.6250\n",
      "Epoch 16/500\n",
      " - 0s - loss: 2.1392 - acc: 0.7500\n",
      "Epoch 17/500\n",
      " - 0s - loss: 2.1353 - acc: 0.7500\n",
      "Epoch 18/500\n",
      " - 0s - loss: 2.1314 - acc: 0.8750\n",
      "Epoch 19/500\n",
      " - 0s - loss: 2.1275 - acc: 0.8750\n",
      "Epoch 20/500\n",
      " - 0s - loss: 2.1236 - acc: 0.8750\n",
      "Epoch 21/500\n",
      " - 0s - loss: 2.1196 - acc: 0.8750\n",
      "Epoch 22/500\n",
      " - 0s - loss: 2.1157 - acc: 0.8750\n",
      "Epoch 23/500\n",
      " - 0s - loss: 2.1116 - acc: 0.8750\n",
      "Epoch 24/500\n",
      " - 0s - loss: 2.1076 - acc: 0.7500\n",
      "Epoch 25/500\n",
      " - 0s - loss: 2.1035 - acc: 0.7500\n",
      "Epoch 26/500\n",
      " - 0s - loss: 2.0994 - acc: 0.7500\n",
      "Epoch 27/500\n",
      " - 0s - loss: 2.0952 - acc: 0.7500\n",
      "Epoch 28/500\n",
      " - 0s - loss: 2.0910 - acc: 0.7500\n",
      "Epoch 29/500\n",
      " - 0s - loss: 2.0868 - acc: 0.7500\n",
      "Epoch 30/500\n",
      " - 0s - loss: 2.0825 - acc: 0.7500\n",
      "Epoch 31/500\n",
      " - 0s - loss: 2.0782 - acc: 0.7500\n",
      "Epoch 32/500\n",
      " - 0s - loss: 2.0738 - acc: 0.7500\n",
      "Epoch 33/500\n",
      " - 0s - loss: 2.0694 - acc: 0.7500\n",
      "Epoch 34/500\n",
      " - 0s - loss: 2.0649 - acc: 0.7500\n",
      "Epoch 35/500\n",
      " - 0s - loss: 2.0604 - acc: 0.7500\n",
      "Epoch 36/500\n",
      " - 0s - loss: 2.0558 - acc: 0.7500\n",
      "Epoch 37/500\n",
      " - 0s - loss: 2.0512 - acc: 0.7500\n",
      "Epoch 38/500\n",
      " - 0s - loss: 2.0465 - acc: 0.7500\n",
      "Epoch 39/500\n",
      " - 0s - loss: 2.0418 - acc: 0.7500\n",
      "Epoch 40/500\n",
      " - 0s - loss: 2.0370 - acc: 0.7500\n",
      "Epoch 41/500\n",
      " - 0s - loss: 2.0321 - acc: 0.7500\n",
      "Epoch 42/500\n",
      " - 0s - loss: 2.0272 - acc: 0.7500\n",
      "Epoch 43/500\n",
      " - 0s - loss: 2.0223 - acc: 0.7500\n",
      "Epoch 44/500\n",
      " - 0s - loss: 2.0173 - acc: 0.7500\n",
      "Epoch 45/500\n",
      " - 0s - loss: 2.0122 - acc: 0.7500\n",
      "Epoch 46/500\n",
      " - 0s - loss: 2.0070 - acc: 0.7500\n",
      "Epoch 47/500\n",
      " - 0s - loss: 2.0018 - acc: 0.7500\n",
      "Epoch 48/500\n",
      " - 0s - loss: 1.9966 - acc: 0.7500\n",
      "Epoch 49/500\n",
      " - 0s - loss: 1.9912 - acc: 0.7500\n",
      "Epoch 50/500\n",
      " - 0s - loss: 1.9858 - acc: 0.7500\n",
      "Epoch 51/500\n",
      " - 0s - loss: 1.9804 - acc: 0.7500\n",
      "Epoch 52/500\n",
      " - 0s - loss: 1.9748 - acc: 0.7500\n",
      "Epoch 53/500\n",
      " - 0s - loss: 1.9693 - acc: 0.7500\n",
      "Epoch 54/500\n",
      " - 0s - loss: 1.9636 - acc: 0.7500\n",
      "Epoch 55/500\n",
      " - 0s - loss: 1.9579 - acc: 0.7500\n",
      "Epoch 56/500\n",
      " - 0s - loss: 1.9521 - acc: 0.7500\n",
      "Epoch 57/500\n",
      " - 0s - loss: 1.9463 - acc: 0.7500\n",
      "Epoch 58/500\n",
      " - 0s - loss: 1.9404 - acc: 0.7500\n",
      "Epoch 59/500\n",
      " - 0s - loss: 1.9344 - acc: 0.7500\n",
      "Epoch 60/500\n",
      " - 0s - loss: 1.9284 - acc: 0.7500\n",
      "Epoch 61/500\n",
      " - 0s - loss: 1.9223 - acc: 0.7500\n",
      "Epoch 62/500\n",
      " - 0s - loss: 1.9161 - acc: 0.7500\n",
      "Epoch 63/500\n",
      " - 0s - loss: 1.9099 - acc: 0.7500\n",
      "Epoch 64/500\n",
      " - 0s - loss: 1.9036 - acc: 0.7500\n",
      "Epoch 65/500\n",
      " - 0s - loss: 1.8972 - acc: 0.7500\n",
      "Epoch 66/500\n",
      " - 0s - loss: 1.8908 - acc: 0.7500\n",
      "Epoch 67/500\n",
      " - 0s - loss: 1.8843 - acc: 0.7500\n",
      "Epoch 68/500\n",
      " - 0s - loss: 1.8777 - acc: 0.7500\n",
      "Epoch 69/500\n",
      " - 0s - loss: 1.8711 - acc: 0.7500\n",
      "Epoch 70/500\n",
      " - 0s - loss: 1.8644 - acc: 0.7500\n",
      "Epoch 71/500\n",
      " - 0s - loss: 1.8577 - acc: 0.7500\n",
      "Epoch 72/500\n",
      " - 0s - loss: 1.8509 - acc: 0.7500\n",
      "Epoch 73/500\n",
      " - 0s - loss: 1.8440 - acc: 0.7500\n",
      "Epoch 74/500\n",
      " - 0s - loss: 1.8371 - acc: 0.7500\n",
      "Epoch 75/500\n",
      " - 0s - loss: 1.8301 - acc: 0.7500\n",
      "Epoch 76/500\n",
      " - 0s - loss: 1.8230 - acc: 0.7500\n",
      "Epoch 77/500\n",
      " - 0s - loss: 1.8159 - acc: 0.7500\n",
      "Epoch 78/500\n",
      " - 0s - loss: 1.8087 - acc: 0.7500\n",
      "Epoch 79/500\n",
      " - 0s - loss: 1.8015 - acc: 0.7500\n",
      "Epoch 80/500\n",
      " - 0s - loss: 1.7942 - acc: 0.7500\n",
      "Epoch 81/500\n",
      " - 0s - loss: 1.7868 - acc: 0.7500\n",
      "Epoch 82/500\n",
      " - 0s - loss: 1.7794 - acc: 0.7500\n",
      "Epoch 83/500\n",
      " - 0s - loss: 1.7720 - acc: 0.7500\n",
      "Epoch 84/500\n",
      " - 0s - loss: 1.7645 - acc: 0.7500\n",
      "Epoch 85/500\n",
      " - 0s - loss: 1.7569 - acc: 0.7500\n",
      "Epoch 86/500\n",
      " - 0s - loss: 1.7493 - acc: 0.7500\n",
      "Epoch 87/500\n",
      " - 0s - loss: 1.7416 - acc: 0.7500\n",
      "Epoch 88/500\n",
      " - 0s - loss: 1.7339 - acc: 0.7500\n",
      "Epoch 89/500\n",
      " - 0s - loss: 1.7261 - acc: 0.7500\n",
      "Epoch 90/500\n",
      " - 0s - loss: 1.7183 - acc: 0.7500\n",
      "Epoch 91/500\n",
      " - 0s - loss: 1.7104 - acc: 0.8750\n",
      "Epoch 92/500\n",
      " - 0s - loss: 1.7025 - acc: 0.8750\n",
      "Epoch 93/500\n",
      " - 0s - loss: 1.6945 - acc: 0.8750\n",
      "Epoch 94/500\n",
      " - 0s - loss: 1.6865 - acc: 0.8750\n",
      "Epoch 95/500\n",
      " - 0s - loss: 1.6785 - acc: 0.8750\n",
      "Epoch 96/500\n",
      " - 0s - loss: 1.6704 - acc: 0.8750\n",
      "Epoch 97/500\n",
      " - 0s - loss: 1.6622 - acc: 0.8750\n",
      "Epoch 98/500\n",
      " - 0s - loss: 1.6540 - acc: 0.8750\n",
      "Epoch 99/500\n",
      " - 0s - loss: 1.6458 - acc: 0.8750\n",
      "Epoch 100/500\n",
      " - 0s - loss: 1.6376 - acc: 0.8750\n",
      "Epoch 101/500\n",
      " - 0s - loss: 1.6293 - acc: 0.8750\n",
      "Epoch 102/500\n",
      " - 0s - loss: 1.6210 - acc: 0.8750\n",
      "Epoch 103/500\n",
      " - 0s - loss: 1.6127 - acc: 0.8750\n",
      "Epoch 104/500\n",
      " - 0s - loss: 1.6043 - acc: 0.8750\n",
      "Epoch 105/500\n",
      " - 0s - loss: 1.5960 - acc: 0.8750\n",
      "Epoch 106/500\n",
      " - 0s - loss: 1.5875 - acc: 0.8750\n",
      "Epoch 107/500\n",
      " - 0s - loss: 1.5791 - acc: 0.8750\n",
      "Epoch 108/500\n",
      " - 0s - loss: 1.5706 - acc: 0.8750\n",
      "Epoch 109/500\n",
      " - 0s - loss: 1.5621 - acc: 0.8750\n",
      "Epoch 110/500\n",
      " - 0s - loss: 1.5536 - acc: 0.8750\n",
      "Epoch 111/500\n",
      " - 0s - loss: 1.5450 - acc: 0.8750\n",
      "Epoch 112/500\n",
      " - 0s - loss: 1.5365 - acc: 0.8750\n",
      "Epoch 113/500\n",
      " - 0s - loss: 1.5279 - acc: 0.8750\n",
      "Epoch 114/500\n",
      " - 0s - loss: 1.5193 - acc: 0.8750\n",
      "Epoch 115/500\n",
      " - 0s - loss: 1.5107 - acc: 0.8750\n",
      "Epoch 116/500\n",
      " - 0s - loss: 1.5021 - acc: 0.8750\n",
      "Epoch 117/500\n",
      " - 0s - loss: 1.4934 - acc: 0.8750\n",
      "Epoch 118/500\n",
      " - 0s - loss: 1.4848 - acc: 0.8750\n",
      "Epoch 119/500\n",
      " - 0s - loss: 1.4761 - acc: 0.8750\n",
      "Epoch 120/500\n",
      " - 0s - loss: 1.4675 - acc: 0.8750\n",
      "Epoch 121/500\n",
      " - 0s - loss: 1.4588 - acc: 0.8750\n",
      "Epoch 122/500\n",
      " - 0s - loss: 1.4501 - acc: 0.8750\n",
      "Epoch 123/500\n",
      " - 0s - loss: 1.4414 - acc: 0.8750\n",
      "Epoch 124/500\n",
      " - 0s - loss: 1.4328 - acc: 0.8750\n",
      "Epoch 125/500\n",
      " - 0s - loss: 1.4241 - acc: 0.8750\n",
      "Epoch 126/500\n",
      " - 0s - loss: 1.4154 - acc: 0.8750\n",
      "Epoch 127/500\n",
      " - 0s - loss: 1.4068 - acc: 0.8750\n",
      "Epoch 128/500\n",
      " - 0s - loss: 1.3981 - acc: 0.8750\n",
      "Epoch 129/500\n",
      " - 0s - loss: 1.3895 - acc: 0.8750\n",
      "Epoch 130/500\n",
      " - 0s - loss: 1.3808 - acc: 0.8750\n",
      "Epoch 131/500\n",
      " - 0s - loss: 1.3722 - acc: 0.8750\n",
      "Epoch 132/500\n",
      " - 0s - loss: 1.3636 - acc: 0.8750\n",
      "Epoch 133/500\n",
      " - 0s - loss: 1.3550 - acc: 0.8750\n",
      "Epoch 134/500\n",
      " - 0s - loss: 1.3464 - acc: 0.8750\n",
      "Epoch 135/500\n",
      " - 0s - loss: 1.3379 - acc: 0.8750\n",
      "Epoch 136/500\n",
      " - 0s - loss: 1.3294 - acc: 0.8750\n",
      "Epoch 137/500\n",
      " - 0s - loss: 1.3208 - acc: 0.8750\n",
      "Epoch 138/500\n",
      " - 0s - loss: 1.3124 - acc: 0.8750\n",
      "Epoch 139/500\n",
      " - 0s - loss: 1.3039 - acc: 0.8750\n",
      "Epoch 140/500\n",
      " - 0s - loss: 1.2955 - acc: 0.8750\n",
      "Epoch 141/500\n",
      " - 0s - loss: 1.2870 - acc: 0.8750\n",
      "Epoch 142/500\n",
      " - 0s - loss: 1.2787 - acc: 0.8750\n",
      "Epoch 143/500\n",
      " - 0s - loss: 1.2703 - acc: 0.8750\n",
      "Epoch 144/500\n",
      " - 0s - loss: 1.2620 - acc: 0.8750\n",
      "Epoch 145/500\n",
      " - 0s - loss: 1.2537 - acc: 0.8750\n",
      "Epoch 146/500\n",
      " - 0s - loss: 1.2455 - acc: 0.8750\n",
      "Epoch 147/500\n",
      " - 0s - loss: 1.2372 - acc: 0.8750\n",
      "Epoch 148/500\n",
      " - 0s - loss: 1.2291 - acc: 0.8750\n",
      "Epoch 149/500\n",
      " - 0s - loss: 1.2209 - acc: 0.8750\n",
      "Epoch 150/500\n",
      " - 0s - loss: 1.2128 - acc: 0.8750\n",
      "Epoch 151/500\n",
      " - 0s - loss: 1.2047 - acc: 0.8750\n",
      "Epoch 152/500\n",
      " - 0s - loss: 1.1967 - acc: 0.8750\n",
      "Epoch 153/500\n",
      " - 0s - loss: 1.1887 - acc: 0.8750\n",
      "Epoch 154/500\n",
      " - 0s - loss: 1.1808 - acc: 0.8750\n",
      "Epoch 155/500\n",
      " - 0s - loss: 1.1728 - acc: 0.8750\n",
      "Epoch 156/500\n",
      " - 0s - loss: 1.1650 - acc: 0.8750\n",
      "Epoch 157/500\n",
      " - 0s - loss: 1.1572 - acc: 0.8750\n",
      "Epoch 158/500\n",
      " - 0s - loss: 1.1494 - acc: 0.8750\n",
      "Epoch 159/500\n",
      " - 0s - loss: 1.1416 - acc: 0.8750\n",
      "Epoch 160/500\n",
      " - 0s - loss: 1.1339 - acc: 0.8750\n",
      "Epoch 161/500\n",
      " - 0s - loss: 1.1263 - acc: 0.8750\n",
      "Epoch 162/500\n",
      " - 0s - loss: 1.1187 - acc: 0.8750\n",
      "Epoch 163/500\n",
      " - 0s - loss: 1.1111 - acc: 0.8750\n",
      "Epoch 164/500\n",
      " - 0s - loss: 1.1036 - acc: 0.8750\n",
      "Epoch 165/500\n",
      " - 0s - loss: 1.0961 - acc: 0.8750\n",
      "Epoch 166/500\n",
      " - 0s - loss: 1.0887 - acc: 0.8750\n",
      "Epoch 167/500\n",
      " - 0s - loss: 1.0813 - acc: 0.8750\n",
      "Epoch 168/500\n",
      " - 0s - loss: 1.0740 - acc: 0.8750\n",
      "Epoch 169/500\n",
      " - 0s - loss: 1.0667 - acc: 0.8750\n",
      "Epoch 170/500\n",
      " - 0s - loss: 1.0595 - acc: 0.8750\n",
      "Epoch 171/500\n",
      " - 0s - loss: 1.0523 - acc: 0.8750\n",
      "Epoch 172/500\n",
      " - 0s - loss: 1.0452 - acc: 0.8750\n",
      "Epoch 173/500\n",
      " - 0s - loss: 1.0381 - acc: 0.8750\n",
      "Epoch 174/500\n",
      " - 0s - loss: 1.0311 - acc: 0.8750\n",
      "Epoch 175/500\n",
      " - 0s - loss: 1.0241 - acc: 0.8750\n",
      "Epoch 176/500\n",
      " - 0s - loss: 1.0171 - acc: 0.8750\n",
      "Epoch 177/500\n",
      " - 0s - loss: 1.0102 - acc: 0.8750\n",
      "Epoch 178/500\n",
      " - 0s - loss: 1.0034 - acc: 0.8750\n",
      "Epoch 179/500\n",
      " - 0s - loss: 0.9966 - acc: 0.8750\n",
      "Epoch 180/500\n",
      " - 0s - loss: 0.9898 - acc: 0.8750\n",
      "Epoch 181/500\n",
      " - 0s - loss: 0.9831 - acc: 0.8750\n",
      "Epoch 182/500\n",
      " - 0s - loss: 0.9765 - acc: 0.8750\n",
      "Epoch 183/500\n",
      " - 0s - loss: 0.9699 - acc: 0.8750\n",
      "Epoch 184/500\n",
      " - 0s - loss: 0.9633 - acc: 0.8750\n",
      "Epoch 185/500\n",
      " - 0s - loss: 0.9568 - acc: 0.8750\n",
      "Epoch 186/500\n",
      " - 0s - loss: 0.9503 - acc: 0.8750\n",
      "Epoch 187/500\n",
      " - 0s - loss: 0.9439 - acc: 0.8750\n",
      "Epoch 188/500\n",
      " - 0s - loss: 0.9375 - acc: 0.8750\n",
      "Epoch 189/500\n",
      " - 0s - loss: 0.9312 - acc: 0.8750\n",
      "Epoch 190/500\n",
      " - 0s - loss: 0.9249 - acc: 0.8750\n",
      "Epoch 191/500\n",
      " - 0s - loss: 0.9187 - acc: 0.8750\n",
      "Epoch 192/500\n",
      " - 0s - loss: 0.9125 - acc: 0.8750\n",
      "Epoch 193/500\n",
      " - 0s - loss: 0.9064 - acc: 0.8750\n",
      "Epoch 194/500\n",
      " - 0s - loss: 0.9003 - acc: 0.8750\n",
      "Epoch 195/500\n",
      " - 0s - loss: 0.8942 - acc: 0.8750\n",
      "Epoch 196/500\n",
      " - 0s - loss: 0.8882 - acc: 0.8750\n",
      "Epoch 197/500\n",
      " - 0s - loss: 0.8823 - acc: 0.8750\n",
      "Epoch 198/500\n",
      " - 0s - loss: 0.8764 - acc: 0.8750\n",
      "Epoch 199/500\n",
      " - 0s - loss: 0.8705 - acc: 0.8750\n",
      "Epoch 200/500\n",
      " - 0s - loss: 0.8647 - acc: 0.8750\n",
      "Epoch 201/500\n",
      " - 0s - loss: 0.8589 - acc: 0.8750\n",
      "Epoch 202/500\n",
      " - 0s - loss: 0.8532 - acc: 0.8750\n",
      "Epoch 203/500\n",
      " - 0s - loss: 0.8475 - acc: 0.8750\n",
      "Epoch 204/500\n",
      " - 0s - loss: 0.8418 - acc: 0.8750\n",
      "Epoch 205/500\n",
      " - 0s - loss: 0.8362 - acc: 0.8750\n",
      "Epoch 206/500\n",
      " - 0s - loss: 0.8307 - acc: 0.8750\n",
      "Epoch 207/500\n",
      " - 0s - loss: 0.8252 - acc: 0.8750\n",
      "Epoch 208/500\n",
      " - 0s - loss: 0.8197 - acc: 0.8750\n",
      "Epoch 209/500\n",
      " - 0s - loss: 0.8143 - acc: 0.8750\n",
      "Epoch 210/500\n",
      " - 0s - loss: 0.8089 - acc: 0.8750\n",
      "Epoch 211/500\n",
      " - 0s - loss: 0.8035 - acc: 0.8750\n",
      "Epoch 212/500\n",
      " - 0s - loss: 0.7982 - acc: 0.8750\n",
      "Epoch 213/500\n",
      " - 0s - loss: 0.7930 - acc: 0.8750\n",
      "Epoch 214/500\n",
      " - 0s - loss: 0.7877 - acc: 0.8750\n",
      "Epoch 215/500\n",
      " - 0s - loss: 0.7826 - acc: 0.8750\n",
      "Epoch 216/500\n",
      " - 0s - loss: 0.7774 - acc: 0.8750\n",
      "Epoch 217/500\n",
      " - 0s - loss: 0.7723 - acc: 0.8750\n",
      "Epoch 218/500\n",
      " - 0s - loss: 0.7672 - acc: 0.8750\n",
      "Epoch 219/500\n",
      " - 0s - loss: 0.7622 - acc: 0.8750\n",
      "Epoch 220/500\n",
      " - 0s - loss: 0.7572 - acc: 0.8750\n",
      "Epoch 221/500\n",
      " - 0s - loss: 0.7523 - acc: 0.8750\n",
      "Epoch 222/500\n",
      " - 0s - loss: 0.7474 - acc: 0.8750\n",
      "Epoch 223/500\n",
      " - 0s - loss: 0.7425 - acc: 0.8750\n",
      "Epoch 224/500\n",
      " - 0s - loss: 0.7376 - acc: 0.8750\n",
      "Epoch 225/500\n",
      " - 0s - loss: 0.7328 - acc: 0.8750\n",
      "Epoch 226/500\n",
      " - 0s - loss: 0.7281 - acc: 0.8750\n",
      "Epoch 227/500\n",
      " - 0s - loss: 0.7234 - acc: 0.8750\n",
      "Epoch 228/500\n",
      " - 0s - loss: 0.7187 - acc: 0.8750\n",
      "Epoch 229/500\n",
      " - 0s - loss: 0.7140 - acc: 0.8750\n",
      "Epoch 230/500\n",
      " - 0s - loss: 0.7094 - acc: 0.8750\n",
      "Epoch 231/500\n",
      " - 0s - loss: 0.7048 - acc: 0.8750\n",
      "Epoch 232/500\n",
      " - 0s - loss: 0.7002 - acc: 0.8750\n",
      "Epoch 233/500\n",
      " - 0s - loss: 0.6957 - acc: 0.8750\n",
      "Epoch 234/500\n",
      " - 0s - loss: 0.6912 - acc: 0.8750\n",
      "Epoch 235/500\n",
      " - 0s - loss: 0.6868 - acc: 0.8750\n",
      "Epoch 236/500\n",
      " - 0s - loss: 0.6824 - acc: 0.8750\n",
      "Epoch 237/500\n",
      " - 0s - loss: 0.6780 - acc: 0.8750\n",
      "Epoch 238/500\n",
      " - 0s - loss: 0.6736 - acc: 0.8750\n",
      "Epoch 239/500\n",
      " - 0s - loss: 0.6693 - acc: 0.8750\n",
      "Epoch 240/500\n",
      " - 0s - loss: 0.6650 - acc: 0.8750\n",
      "Epoch 241/500\n",
      " - 0s - loss: 0.6608 - acc: 0.8750\n",
      "Epoch 242/500\n",
      " - 0s - loss: 0.6566 - acc: 0.8750\n",
      "Epoch 243/500\n",
      " - 0s - loss: 0.6524 - acc: 0.8750\n",
      "Epoch 244/500\n",
      " - 0s - loss: 0.6482 - acc: 0.8750\n",
      "Epoch 245/500\n",
      " - 0s - loss: 0.6441 - acc: 0.8750\n",
      "Epoch 246/500\n",
      " - 0s - loss: 0.6400 - acc: 0.8750\n",
      "Epoch 247/500\n",
      " - 0s - loss: 0.6359 - acc: 0.8750\n",
      "Epoch 248/500\n",
      " - 0s - loss: 0.6319 - acc: 0.8750\n",
      "Epoch 249/500\n",
      " - 0s - loss: 0.6279 - acc: 0.8750\n",
      "Epoch 250/500\n",
      " - 0s - loss: 0.6239 - acc: 0.8750\n",
      "Epoch 251/500\n",
      " - 0s - loss: 0.6200 - acc: 0.8750\n",
      "Epoch 252/500\n",
      " - 0s - loss: 0.6161 - acc: 0.8750\n",
      "Epoch 253/500\n",
      " - 0s - loss: 0.6122 - acc: 0.8750\n",
      "Epoch 254/500\n",
      " - 0s - loss: 0.6084 - acc: 0.8750\n",
      "Epoch 255/500\n",
      " - 0s - loss: 0.6046 - acc: 0.8750\n",
      "Epoch 256/500\n",
      " - 0s - loss: 0.6008 - acc: 0.8750\n",
      "Epoch 257/500\n",
      " - 0s - loss: 0.5970 - acc: 0.8750\n",
      "Epoch 258/500\n",
      " - 0s - loss: 0.5933 - acc: 0.8750\n",
      "Epoch 259/500\n",
      " - 0s - loss: 0.5896 - acc: 0.8750\n",
      "Epoch 260/500\n",
      " - 0s - loss: 0.5859 - acc: 0.8750\n",
      "Epoch 261/500\n",
      " - 0s - loss: 0.5823 - acc: 0.8750\n",
      "Epoch 262/500\n",
      " - 0s - loss: 0.5787 - acc: 0.8750\n",
      "Epoch 263/500\n",
      " - 0s - loss: 0.5751 - acc: 0.8750\n",
      "Epoch 264/500\n",
      " - 0s - loss: 0.5715 - acc: 0.8750\n",
      "Epoch 265/500\n",
      " - 0s - loss: 0.5680 - acc: 0.8750\n",
      "Epoch 266/500\n",
      " - 0s - loss: 0.5645 - acc: 0.8750\n",
      "Epoch 267/500\n",
      " - 0s - loss: 0.5610 - acc: 0.8750\n",
      "Epoch 268/500\n",
      " - 0s - loss: 0.5576 - acc: 0.8750\n",
      "Epoch 269/500\n",
      " - 0s - loss: 0.5542 - acc: 0.8750\n",
      "Epoch 270/500\n",
      " - 0s - loss: 0.5508 - acc: 0.8750\n",
      "Epoch 271/500\n",
      " - 0s - loss: 0.5475 - acc: 0.8750\n",
      "Epoch 272/500\n",
      " - 0s - loss: 0.5441 - acc: 0.8750\n",
      "Epoch 273/500\n",
      " - 0s - loss: 0.5409 - acc: 0.8750\n",
      "Epoch 274/500\n",
      " - 0s - loss: 0.5376 - acc: 0.8750\n",
      "Epoch 275/500\n",
      " - 0s - loss: 0.5343 - acc: 0.8750\n",
      "Epoch 276/500\n",
      " - 0s - loss: 0.5311 - acc: 0.8750\n",
      "Epoch 277/500\n",
      " - 0s - loss: 0.5280 - acc: 0.8750\n",
      "Epoch 278/500\n",
      " - 0s - loss: 0.5248 - acc: 0.8750\n",
      "Epoch 279/500\n",
      " - 0s - loss: 0.5217 - acc: 0.8750\n",
      "Epoch 280/500\n",
      " - 0s - loss: 0.5186 - acc: 0.8750\n",
      "Epoch 281/500\n",
      " - 0s - loss: 0.5155 - acc: 0.8750\n",
      "Epoch 282/500\n",
      " - 0s - loss: 0.5125 - acc: 0.8750\n",
      "Epoch 283/500\n",
      " - 0s - loss: 0.5095 - acc: 0.8750\n",
      "Epoch 284/500\n",
      " - 0s - loss: 0.5065 - acc: 0.8750\n",
      "Epoch 285/500\n",
      " - 0s - loss: 0.5035 - acc: 0.8750\n",
      "Epoch 286/500\n",
      " - 0s - loss: 0.5006 - acc: 0.8750\n",
      "Epoch 287/500\n",
      " - 0s - loss: 0.4977 - acc: 0.8750\n",
      "Epoch 288/500\n",
      " - 0s - loss: 0.4948 - acc: 0.8750\n",
      "Epoch 289/500\n",
      " - 0s - loss: 0.4920 - acc: 0.8750\n",
      "Epoch 290/500\n",
      " - 0s - loss: 0.4891 - acc: 0.8750\n",
      "Epoch 291/500\n",
      " - 0s - loss: 0.4863 - acc: 0.8750\n",
      "Epoch 292/500\n",
      " - 0s - loss: 0.4836 - acc: 0.8750\n",
      "Epoch 293/500\n",
      " - 0s - loss: 0.4808 - acc: 0.8750\n",
      "Epoch 294/500\n",
      " - 0s - loss: 0.4781 - acc: 0.8750\n",
      "Epoch 295/500\n",
      " - 0s - loss: 0.4754 - acc: 0.8750\n",
      "Epoch 296/500\n",
      " - 0s - loss: 0.4728 - acc: 0.8750\n",
      "Epoch 297/500\n",
      " - 0s - loss: 0.4701 - acc: 0.8750\n",
      "Epoch 298/500\n",
      " - 0s - loss: 0.4675 - acc: 0.8750\n",
      "Epoch 299/500\n",
      " - 0s - loss: 0.4649 - acc: 0.8750\n",
      "Epoch 300/500\n",
      " - 0s - loss: 0.4624 - acc: 0.8750\n",
      "Epoch 301/500\n",
      " - 0s - loss: 0.4599 - acc: 0.8750\n",
      "Epoch 302/500\n",
      " - 0s - loss: 0.4573 - acc: 0.8750\n",
      "Epoch 303/500\n",
      " - 0s - loss: 0.4549 - acc: 0.8750\n",
      "Epoch 304/500\n",
      " - 0s - loss: 0.4524 - acc: 0.8750\n",
      "Epoch 305/500\n",
      " - 0s - loss: 0.4500 - acc: 0.8750\n",
      "Epoch 306/500\n",
      " - 0s - loss: 0.4476 - acc: 0.8750\n",
      "Epoch 307/500\n",
      " - 0s - loss: 0.4452 - acc: 0.8750\n",
      "Epoch 308/500\n",
      " - 0s - loss: 0.4429 - acc: 0.8750\n",
      "Epoch 309/500\n",
      " - 0s - loss: 0.4405 - acc: 0.8750\n",
      "Epoch 310/500\n",
      " - 0s - loss: 0.4382 - acc: 0.8750\n",
      "Epoch 311/500\n",
      " - 0s - loss: 0.4359 - acc: 0.8750\n",
      "Epoch 312/500\n",
      " - 0s - loss: 0.4337 - acc: 0.8750\n",
      "Epoch 313/500\n",
      " - 0s - loss: 0.4315 - acc: 0.8750\n",
      "Epoch 314/500\n",
      " - 0s - loss: 0.4293 - acc: 0.8750\n",
      "Epoch 315/500\n",
      " - 0s - loss: 0.4271 - acc: 0.8750\n",
      "Epoch 316/500\n",
      " - 0s - loss: 0.4249 - acc: 0.8750\n",
      "Epoch 317/500\n",
      " - 0s - loss: 0.4228 - acc: 0.8750\n",
      "Epoch 318/500\n",
      " - 0s - loss: 0.4207 - acc: 0.8750\n",
      "Epoch 319/500\n",
      " - 0s - loss: 0.4186 - acc: 0.8750\n",
      "Epoch 320/500\n",
      " - 0s - loss: 0.4165 - acc: 0.8750\n",
      "Epoch 321/500\n",
      " - 0s - loss: 0.4145 - acc: 0.8750\n",
      "Epoch 322/500\n",
      " - 0s - loss: 0.4124 - acc: 0.8750\n",
      "Epoch 323/500\n",
      " - 0s - loss: 0.4104 - acc: 0.8750\n",
      "Epoch 324/500\n",
      " - 0s - loss: 0.4085 - acc: 0.8750\n",
      "Epoch 325/500\n",
      " - 0s - loss: 0.4065 - acc: 0.8750\n",
      "Epoch 326/500\n",
      " - 0s - loss: 0.4046 - acc: 0.8750\n",
      "Epoch 327/500\n",
      " - 0s - loss: 0.4027 - acc: 0.8750\n",
      "Epoch 328/500\n",
      " - 0s - loss: 0.4008 - acc: 0.8750\n",
      "Epoch 329/500\n",
      " - 0s - loss: 0.3989 - acc: 0.8750\n",
      "Epoch 330/500\n",
      " - 0s - loss: 0.3970 - acc: 0.8750\n",
      "Epoch 331/500\n",
      " - 0s - loss: 0.3952 - acc: 0.8750\n",
      "Epoch 332/500\n",
      " - 0s - loss: 0.3934 - acc: 0.8750\n",
      "Epoch 333/500\n",
      " - 0s - loss: 0.3916 - acc: 0.8750\n",
      "Epoch 334/500\n",
      " - 0s - loss: 0.3899 - acc: 0.8750\n",
      "Epoch 335/500\n",
      " - 0s - loss: 0.3881 - acc: 0.8750\n",
      "Epoch 336/500\n",
      " - 0s - loss: 0.3864 - acc: 0.8750\n",
      "Epoch 337/500\n",
      " - 0s - loss: 0.3847 - acc: 0.8750\n",
      "Epoch 338/500\n",
      " - 0s - loss: 0.3830 - acc: 0.8750\n",
      "Epoch 339/500\n",
      " - 0s - loss: 0.3813 - acc: 0.8750\n",
      "Epoch 340/500\n",
      " - 0s - loss: 0.3796 - acc: 0.8750\n",
      "Epoch 341/500\n",
      " - 0s - loss: 0.3780 - acc: 0.8750\n",
      "Epoch 342/500\n",
      " - 0s - loss: 0.3764 - acc: 0.8750\n",
      "Epoch 343/500\n",
      " - 0s - loss: 0.3748 - acc: 0.8750\n",
      "Epoch 344/500\n",
      " - 0s - loss: 0.3732 - acc: 0.8750\n",
      "Epoch 345/500\n",
      " - 0s - loss: 0.3717 - acc: 0.8750\n",
      "Epoch 346/500\n",
      " - 0s - loss: 0.3701 - acc: 0.8750\n",
      "Epoch 347/500\n",
      " - 0s - loss: 0.3686 - acc: 0.8750\n",
      "Epoch 348/500\n",
      " - 0s - loss: 0.3671 - acc: 0.8750\n",
      "Epoch 349/500\n",
      " - 0s - loss: 0.3656 - acc: 0.8750\n",
      "Epoch 350/500\n",
      " - 0s - loss: 0.3641 - acc: 0.8750\n",
      "Epoch 351/500\n",
      " - 0s - loss: 0.3626 - acc: 0.8750\n",
      "Epoch 352/500\n",
      " - 0s - loss: 0.3612 - acc: 0.8750\n",
      "Epoch 353/500\n",
      " - 0s - loss: 0.3598 - acc: 0.8750\n",
      "Epoch 354/500\n",
      " - 0s - loss: 0.3583 - acc: 0.8750\n",
      "Epoch 355/500\n",
      " - 0s - loss: 0.3569 - acc: 0.8750\n",
      "Epoch 356/500\n",
      " - 0s - loss: 0.3556 - acc: 0.8750\n",
      "Epoch 357/500\n",
      " - 0s - loss: 0.3542 - acc: 0.8750\n",
      "Epoch 358/500\n",
      " - 0s - loss: 0.3528 - acc: 0.8750\n",
      "Epoch 359/500\n",
      " - 0s - loss: 0.3515 - acc: 0.8750\n",
      "Epoch 360/500\n",
      " - 0s - loss: 0.3502 - acc: 0.8750\n",
      "Epoch 361/500\n",
      " - 0s - loss: 0.3489 - acc: 0.8750\n",
      "Epoch 362/500\n",
      " - 0s - loss: 0.3476 - acc: 0.8750\n",
      "Epoch 363/500\n",
      " - 0s - loss: 0.3463 - acc: 0.8750\n",
      "Epoch 364/500\n",
      " - 0s - loss: 0.3450 - acc: 0.8750\n",
      "Epoch 365/500\n",
      " - 0s - loss: 0.3438 - acc: 0.8750\n",
      "Epoch 366/500\n",
      " - 0s - loss: 0.3426 - acc: 0.8750\n",
      "Epoch 367/500\n",
      " - 0s - loss: 0.3413 - acc: 0.8750\n",
      "Epoch 368/500\n",
      " - 0s - loss: 0.3401 - acc: 0.8750\n",
      "Epoch 369/500\n",
      " - 0s - loss: 0.3389 - acc: 0.8750\n",
      "Epoch 370/500\n",
      " - 0s - loss: 0.3377 - acc: 0.8750\n",
      "Epoch 371/500\n",
      " - 0s - loss: 0.3366 - acc: 0.8750\n",
      "Epoch 372/500\n",
      " - 0s - loss: 0.3354 - acc: 0.8750\n",
      "Epoch 373/500\n",
      " - 0s - loss: 0.3343 - acc: 0.8750\n",
      "Epoch 374/500\n",
      " - 0s - loss: 0.3331 - acc: 0.8750\n",
      "Epoch 375/500\n",
      " - 0s - loss: 0.3320 - acc: 0.8750\n",
      "Epoch 376/500\n",
      " - 0s - loss: 0.3309 - acc: 0.8750\n",
      "Epoch 377/500\n",
      " - 0s - loss: 0.3298 - acc: 0.8750\n",
      "Epoch 378/500\n",
      " - 0s - loss: 0.3287 - acc: 0.8750\n",
      "Epoch 379/500\n",
      " - 0s - loss: 0.3276 - acc: 0.8750\n",
      "Epoch 380/500\n",
      " - 0s - loss: 0.3266 - acc: 0.8750\n",
      "Epoch 381/500\n",
      " - 0s - loss: 0.3255 - acc: 0.8750\n",
      "Epoch 382/500\n",
      " - 0s - loss: 0.3245 - acc: 0.8750\n",
      "Epoch 383/500\n",
      " - 0s - loss: 0.3234 - acc: 0.8750\n",
      "Epoch 384/500\n",
      " - 0s - loss: 0.3224 - acc: 0.8750\n",
      "Epoch 385/500\n",
      " - 0s - loss: 0.3214 - acc: 0.8750\n",
      "Epoch 386/500\n",
      " - 0s - loss: 0.3204 - acc: 0.8750\n",
      "Epoch 387/500\n",
      " - 0s - loss: 0.3194 - acc: 0.8750\n",
      "Epoch 388/500\n",
      " - 0s - loss: 0.3184 - acc: 0.8750\n",
      "Epoch 389/500\n",
      " - 0s - loss: 0.3175 - acc: 0.8750\n",
      "Epoch 390/500\n",
      " - 0s - loss: 0.3165 - acc: 0.8750\n",
      "Epoch 391/500\n",
      " - 0s - loss: 0.3156 - acc: 0.8750\n",
      "Epoch 392/500\n",
      " - 0s - loss: 0.3146 - acc: 0.8750\n",
      "Epoch 393/500\n",
      " - 0s - loss: 0.3137 - acc: 0.8750\n",
      "Epoch 394/500\n",
      " - 0s - loss: 0.3128 - acc: 0.8750\n",
      "Epoch 395/500\n",
      " - 0s - loss: 0.3119 - acc: 0.8750\n",
      "Epoch 396/500\n",
      " - 0s - loss: 0.3110 - acc: 0.8750\n",
      "Epoch 397/500\n",
      " - 0s - loss: 0.3101 - acc: 0.8750\n",
      "Epoch 398/500\n",
      " - 0s - loss: 0.3092 - acc: 0.8750\n",
      "Epoch 399/500\n",
      " - 0s - loss: 0.3083 - acc: 0.8750\n",
      "Epoch 400/500\n",
      " - 0s - loss: 0.3074 - acc: 0.8750\n",
      "Epoch 401/500\n",
      " - 0s - loss: 0.3066 - acc: 0.8750\n",
      "Epoch 402/500\n",
      " - 0s - loss: 0.3057 - acc: 0.8750\n",
      "Epoch 403/500\n",
      " - 0s - loss: 0.3049 - acc: 0.8750\n",
      "Epoch 404/500\n",
      " - 0s - loss: 0.3041 - acc: 0.8750\n",
      "Epoch 405/500\n",
      " - 0s - loss: 0.3032 - acc: 0.8750\n",
      "Epoch 406/500\n",
      " - 0s - loss: 0.3024 - acc: 0.8750\n",
      "Epoch 407/500\n",
      " - 0s - loss: 0.3016 - acc: 0.8750\n",
      "Epoch 408/500\n",
      " - 0s - loss: 0.3008 - acc: 0.8750\n",
      "Epoch 409/500\n",
      " - 0s - loss: 0.3000 - acc: 0.8750\n",
      "Epoch 410/500\n",
      " - 0s - loss: 0.2992 - acc: 0.8750\n",
      "Epoch 411/500\n",
      " - 0s - loss: 0.2984 - acc: 0.8750\n",
      "Epoch 412/500\n",
      " - 0s - loss: 0.2977 - acc: 0.8750\n",
      "Epoch 413/500\n",
      " - 0s - loss: 0.2969 - acc: 0.8750\n",
      "Epoch 414/500\n",
      " - 0s - loss: 0.2962 - acc: 0.8750\n",
      "Epoch 415/500\n",
      " - 0s - loss: 0.2954 - acc: 0.8750\n",
      "Epoch 416/500\n",
      " - 0s - loss: 0.2947 - acc: 0.8750\n",
      "Epoch 417/500\n",
      " - 0s - loss: 0.2939 - acc: 0.8750\n",
      "Epoch 418/500\n",
      " - 0s - loss: 0.2932 - acc: 0.8750\n",
      "Epoch 419/500\n",
      " - 0s - loss: 0.2925 - acc: 0.8750\n",
      "Epoch 420/500\n",
      " - 0s - loss: 0.2918 - acc: 0.8750\n",
      "Epoch 421/500\n",
      " - 0s - loss: 0.2911 - acc: 0.8750\n",
      "Epoch 422/500\n",
      " - 0s - loss: 0.2904 - acc: 0.8750\n",
      "Epoch 423/500\n",
      " - 0s - loss: 0.2897 - acc: 0.8750\n",
      "Epoch 424/500\n",
      " - 0s - loss: 0.2890 - acc: 0.8750\n",
      "Epoch 425/500\n",
      " - 0s - loss: 0.2883 - acc: 0.8750\n",
      "Epoch 426/500\n",
      " - 0s - loss: 0.2876 - acc: 0.8750\n",
      "Epoch 427/500\n",
      " - 0s - loss: 0.2870 - acc: 0.8750\n",
      "Epoch 428/500\n",
      " - 0s - loss: 0.2863 - acc: 0.8750\n",
      "Epoch 429/500\n",
      " - 0s - loss: 0.2857 - acc: 0.8750\n",
      "Epoch 430/500\n",
      " - 0s - loss: 0.2850 - acc: 0.8750\n",
      "Epoch 431/500\n",
      " - 0s - loss: 0.2844 - acc: 0.8750\n",
      "Epoch 432/500\n",
      " - 0s - loss: 0.2837 - acc: 0.8750\n",
      "Epoch 433/500\n",
      " - 0s - loss: 0.2831 - acc: 0.8750\n",
      "Epoch 434/500\n",
      " - 0s - loss: 0.2825 - acc: 0.8750\n",
      "Epoch 435/500\n",
      " - 0s - loss: 0.2818 - acc: 0.8750\n",
      "Epoch 436/500\n",
      " - 0s - loss: 0.2812 - acc: 0.8750\n",
      "Epoch 437/500\n",
      " - 0s - loss: 0.2806 - acc: 0.8750\n",
      "Epoch 438/500\n",
      " - 0s - loss: 0.2800 - acc: 0.8750\n",
      "Epoch 439/500\n",
      " - 0s - loss: 0.2794 - acc: 0.8750\n",
      "Epoch 440/500\n",
      " - 0s - loss: 0.2788 - acc: 0.8750\n",
      "Epoch 441/500\n",
      " - 0s - loss: 0.2782 - acc: 0.8750\n",
      "Epoch 442/500\n",
      " - 0s - loss: 0.2776 - acc: 0.8750\n",
      "Epoch 443/500\n",
      " - 0s - loss: 0.2771 - acc: 0.8750\n",
      "Epoch 444/500\n",
      " - 0s - loss: 0.2765 - acc: 0.8750\n",
      "Epoch 445/500\n",
      " - 0s - loss: 0.2759 - acc: 0.8750\n",
      "Epoch 446/500\n",
      " - 0s - loss: 0.2754 - acc: 0.8750\n",
      "Epoch 447/500\n",
      " - 0s - loss: 0.2748 - acc: 0.8750\n",
      "Epoch 448/500\n",
      " - 0s - loss: 0.2742 - acc: 0.8750\n",
      "Epoch 449/500\n",
      " - 0s - loss: 0.2737 - acc: 0.8750\n",
      "Epoch 450/500\n",
      " - 0s - loss: 0.2732 - acc: 0.8750\n",
      "Epoch 451/500\n",
      " - 0s - loss: 0.2726 - acc: 0.8750\n",
      "Epoch 452/500\n",
      " - 0s - loss: 0.2721 - acc: 0.8750\n",
      "Epoch 453/500\n",
      " - 0s - loss: 0.2715 - acc: 0.8750\n",
      "Epoch 454/500\n",
      " - 0s - loss: 0.2710 - acc: 0.8750\n",
      "Epoch 455/500\n",
      " - 0s - loss: 0.2705 - acc: 0.8750\n",
      "Epoch 456/500\n",
      " - 0s - loss: 0.2700 - acc: 0.8750\n",
      "Epoch 457/500\n",
      " - 0s - loss: 0.2695 - acc: 0.8750\n",
      "Epoch 458/500\n",
      " - 0s - loss: 0.2690 - acc: 0.8750\n",
      "Epoch 459/500\n",
      " - 0s - loss: 0.2685 - acc: 0.8750\n",
      "Epoch 460/500\n",
      " - 0s - loss: 0.2680 - acc: 0.8750\n",
      "Epoch 461/500\n",
      " - 0s - loss: 0.2675 - acc: 0.8750\n",
      "Epoch 462/500\n",
      " - 0s - loss: 0.2670 - acc: 0.8750\n",
      "Epoch 463/500\n",
      " - 0s - loss: 0.2665 - acc: 0.8750\n",
      "Epoch 464/500\n",
      " - 0s - loss: 0.2660 - acc: 0.8750\n",
      "Epoch 465/500\n",
      " - 0s - loss: 0.2655 - acc: 0.8750\n",
      "Epoch 466/500\n",
      " - 0s - loss: 0.2650 - acc: 0.8750\n",
      "Epoch 467/500\n",
      " - 0s - loss: 0.2646 - acc: 0.8750\n",
      "Epoch 468/500\n",
      " - 0s - loss: 0.2641 - acc: 0.8750\n",
      "Epoch 469/500\n",
      " - 0s - loss: 0.2636 - acc: 0.8750\n",
      "Epoch 470/500\n",
      " - 0s - loss: 0.2632 - acc: 0.8750\n",
      "Epoch 471/500\n",
      " - 0s - loss: 0.2627 - acc: 0.8750\n",
      "Epoch 472/500\n",
      " - 0s - loss: 0.2623 - acc: 0.8750\n",
      "Epoch 473/500\n",
      " - 0s - loss: 0.2618 - acc: 0.8750\n",
      "Epoch 474/500\n",
      " - 0s - loss: 0.2614 - acc: 0.8750\n",
      "Epoch 475/500\n",
      " - 0s - loss: 0.2609 - acc: 0.8750\n",
      "Epoch 476/500\n",
      " - 0s - loss: 0.2605 - acc: 0.8750\n",
      "Epoch 477/500\n",
      " - 0s - loss: 0.2601 - acc: 0.8750\n",
      "Epoch 478/500\n",
      " - 0s - loss: 0.2596 - acc: 0.8750\n",
      "Epoch 479/500\n",
      " - 0s - loss: 0.2592 - acc: 0.8750\n",
      "Epoch 480/500\n",
      " - 0s - loss: 0.2588 - acc: 0.8750\n",
      "Epoch 481/500\n",
      " - 0s - loss: 0.2583 - acc: 0.8750\n",
      "Epoch 482/500\n",
      " - 0s - loss: 0.2579 - acc: 0.8750\n",
      "Epoch 483/500\n",
      " - 0s - loss: 0.2575 - acc: 0.8750\n",
      "Epoch 484/500\n",
      " - 0s - loss: 0.2571 - acc: 0.8750\n",
      "Epoch 485/500\n",
      " - 0s - loss: 0.2567 - acc: 0.8750\n",
      "Epoch 486/500\n",
      " - 0s - loss: 0.2563 - acc: 0.8750\n",
      "Epoch 487/500\n",
      " - 0s - loss: 0.2559 - acc: 0.8750\n",
      "Epoch 488/500\n",
      " - 0s - loss: 0.2555 - acc: 0.8750\n",
      "Epoch 489/500\n",
      " - 0s - loss: 0.2551 - acc: 0.8750\n",
      "Epoch 490/500\n",
      " - 0s - loss: 0.2547 - acc: 0.8750\n",
      "Epoch 491/500\n",
      " - 0s - loss: 0.2543 - acc: 0.8750\n",
      "Epoch 492/500\n",
      " - 0s - loss: 0.2539 - acc: 0.8750\n",
      "Epoch 493/500\n",
      " - 0s - loss: 0.2535 - acc: 0.8750\n",
      "Epoch 494/500\n",
      " - 0s - loss: 0.2532 - acc: 0.8750\n",
      "Epoch 495/500\n",
      " - 0s - loss: 0.2528 - acc: 0.8750\n",
      "Epoch 496/500\n",
      " - 0s - loss: 0.2524 - acc: 0.8750\n",
      "Epoch 497/500\n",
      " - 0s - loss: 0.2520 - acc: 0.8750\n",
      "Epoch 498/500\n",
      " - 0s - loss: 0.2517 - acc: 0.8750\n",
      "Epoch 499/500\n",
      " - 0s - loss: 0.2513 - acc: 0.8750\n",
      "Epoch 500/500\n",
      " - 0s - loss: 0.2509 - acc: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f209cbacfd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(X,y,epochs=500,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IWltBJstJA3y"
   },
   "source": [
    "**입력한 단어에 대해 다음 단어를 정상적으로 예측하는지 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3738,
     "status": "ok",
     "timestamp": 1566355642722,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "aEptBZo0C8Pl",
    "outputId": "ff581e3d-c311-4938-86e2-9e14b58b3d95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('점심', 1), ('나랑', 2), ('먹으로', 3), ('갈래', 4), ('메뉴는', 5), ('햄버거', 6), ('메뉴', 7), ('좋지', 8)])\n"
     ]
    }
   ],
   "source": [
    "print(t.word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vGcnVT-Iyt1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CUL6T_ioI0lg"
   },
   "source": [
    "\n",
    "\n",
    "> 다음 단어를 출력하는 함수\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4VaU1nWDC8Pn"
   },
   "outputs": [],
   "source": [
    "def predict_next_word(model, t, current_word):\n",
    "    encoded = t.texts_to_sequences([current_word])[0]\n",
    "    encoded = np.array(encoded)\n",
    "    result = model.predict_classes(encoded, verbose=0)\n",
    "    for word, index in t.word_index.items():\n",
    "        if index == result:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_AYm8nIHI6GJ"
   },
   "source": [
    "주어진 단어로 부터 문장을 생성하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGVaEeOAC8Pq"
   },
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n):\n",
    "    init_word = current_word\n",
    "    sentence = \"\"\n",
    "    for _ in range(n):\n",
    "        encoded = t.texts_to_sequences([current_word])[0]\n",
    "        encoded = np.array(encoded)\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "        for word, index in t.word_index.items():\n",
    "            if index == result:\n",
    "                break\n",
    "        current_word = word\n",
    "        sentence = sentence + \" \" + word\n",
    "    \n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1iUIaO7okRbY"
   },
   "source": [
    "# **문맥을 반영해서 다음 단어 예측**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJZPoIuZkKvz"
   },
   "outputs": [],
   "source": [
    "text=\"\"\"경마장에 있는 말이 뛰고 있다\\n\n",
    "그의 말이 법이다\\n\n",
    "가는 말이 고와야 오는 말이 곱다\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-IBQxMXhkhZq"
   },
   "outputs": [],
   "source": [
    "from keras_preprocessing.text import Tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts([text])\n",
    "encoded = t.texts_to_sequences([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1566364682699,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "RM7cx4dSm03A",
    "outputId": "e0117ede-36a7-4c15-d215-b01673aaeac0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 1, 4, 5, 6, 1, 7, 8, 1, 9, 10, 1, 11]"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vntvzyH3n5iF"
   },
   "source": [
    "# 토큰화와 정수 인코딩 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 767,
     "status": "ok",
     "timestamp": 1566364042160,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "zpnQcfThklHo",
    "outputId": "cea7cfd6-b60e-4ba3-8f79-9484a5ecbc00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 12\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(t.word_index) + 1\n",
    "# 케라스 토크나이저의 정수 인코딩은 인덱스가 1부터 시작하지만,\n",
    "# 케라스 원-핫 인코딩에서 배열의 인덱스가 0부터 시작하기 때문에\n",
    "# 배열의 크기를 실제 단어 집합의 크기보다 +1로 생성해야하므로 미리 +1 선언 \n",
    "print('단어 집합의 크기 : %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_vlm37tonT44"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 739,
     "status": "ok",
     "timestamp": 1566364048345,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "s_qcciMbkj0v",
    "outputId": "b369f699-9bf1-40f2-e8b3-c90a4e49bd9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'말이': 1, '경마장에': 2, '있는': 3, '뛰고': 4, '있다': 5, '그의': 6, '법이다': 7, '가는': 8, '고와야': 9, '오는': 10, '곱다': 11}\n"
     ]
    }
   ],
   "source": [
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rjqksNPVn9xv"
   },
   "source": [
    "# 훈련 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 859,
     "status": "ok",
     "timestamp": 1566364068802,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "yuZQPaHYkpi4",
    "outputId": "c51db833-f715-4333-9cda-2ddb7be28395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습에 사용할 샘플의 개수: 11\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for line in text.split('\\n'): # Wn을 기준으로 문장 토큰화\n",
    "    encoded = t.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "print('학습에 사용할 샘플의 개수: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 764,
     "status": "ok",
     "timestamp": 1566364839539,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "-qLS-fZAno2n",
    "outputId": "938c5c6b-1fb8-4f75-bf0e-abd91ccece30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 1, 4, 5, 6, 1, 7, 8, 1, 9, 10, 1, 11]"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rfBNZisioYsX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 734,
     "status": "ok",
     "timestamp": 1566364073232,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "omKfZbkdkrdK",
    "outputId": "d1ac2c16-9fb3-4d9b-e604-9d7c1af3d98a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 656,
     "status": "ok",
     "timestamp": 1566364078963,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "YJ9PH9Wmkvn3",
    "outputId": "4f4d37f0-79c9-4ef4-f7e6-20c31edf2837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(max(len(l) for l in sequences)) # 모든 샘플에서 길이가 가장 긴 샘플의 길이 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ldm1zidhpFon"
   },
   "source": [
    "전체 훈련 데이터에서 가장 긴 샘플의 길이가 6임을 확인하였습니다. 이제 전체 샘플의 길이를 6으로 패딩합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kI-OOZgqkxCj"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_len=6\n",
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AeZTFrdApWWv"
   },
   "source": [
    "maxlen의 값으로 6을 주면 모든 샘플의 길이를 6으로 맞춰주며, padding의 인자로 'pre'를 주면 길이가 6보다 짧은 샘플의 앞에 0으로 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1566364093993,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "h8QR1e4dkzAT",
    "outputId": "0f5f2559-fc90-483f-c949-6e4f4d300668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  2  3]\n",
      " [ 0  0  0  2  3  1]\n",
      " [ 0  0  2  3  1  4]\n",
      " [ 0  2  3  1  4  5]\n",
      " [ 0  0  0  0  6  1]\n",
      " [ 0  0  0  6  1  7]\n",
      " [ 0  0  0  0  8  1]\n",
      " [ 0  0  0  8  1  9]\n",
      " [ 0  0  8  1  9 10]\n",
      " [ 0  8  1  9 10  1]\n",
      " [ 8  1  9 10  1 11]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fai-XpowqBon"
   },
   "source": [
    "각 샘플의 마지막 단어를 레이블로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zdDJSzyKk0re"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sequences = np.array(sequences)\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "# 리스트의 마지막 값을 제외하고 저장한 것은 X\n",
    "# 리스트의 마지막 값만 저장한 것은 y. 이는 레이블에 해당됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 757,
     "status": "ok",
     "timestamp": 1566364110324,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "JxgxWmJmk2Ix",
    "outputId": "585e25ac-9d90-4ef0-c5fe-6bb1032223c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  2]\n",
      " [ 0  0  0  2  3]\n",
      " [ 0  0  2  3  1]\n",
      " [ 0  2  3  1  4]\n",
      " [ 0  0  0  0  6]\n",
      " [ 0  0  0  6  1]\n",
      " [ 0  0  0  0  8]\n",
      " [ 0  0  0  8  1]\n",
      " [ 0  0  8  1  9]\n",
      " [ 0  8  1  9 10]\n",
      " [ 8  1  9 10  1]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 766,
     "status": "ok",
     "timestamp": 1566364116486,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "XCenTdwek4rK",
    "outputId": "478e6cf1-1a7a-4537-97fc-ddbda6a77737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  1  4  5  1  7  1  9 10  1 11]\n"
     ]
    }
   ],
   "source": [
    "print(y) # 모든 샘플에 대한 레이블 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krZ_bZQPtEIP"
   },
   "source": [
    "이제 RNN 모델에 훈련 데이터를 훈련 시키기 전에 레이블에 대해서 원-핫 인코딩을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0QHR0slFk6LR"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 764,
     "status": "ok",
     "timestamp": 1566364130689,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "q1SlXz39k7md",
    "outputId": "f61d0da7-9364-4ac0-d8c2-494e1aad58c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BR_b8K8ytIT3"
   },
   "source": [
    "# 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2943,
     "status": "ok",
     "timestamp": 1566364139808,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "AltLxbeok9pU",
    "outputId": "38f2dac6-688c-45bc-cc37-d51a2a2f2a1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 0s - loss: 2.4510 - acc: 0.2727\n",
      "Epoch 2/200\n",
      " - 0s - loss: 2.4375 - acc: 0.3636\n",
      "Epoch 3/200\n",
      " - 0s - loss: 2.4237 - acc: 0.5455\n",
      "Epoch 4/200\n",
      " - 0s - loss: 2.4095 - acc: 0.5455\n",
      "Epoch 5/200\n",
      " - 0s - loss: 2.3948 - acc: 0.5455\n",
      "Epoch 6/200\n",
      " - 0s - loss: 2.3795 - acc: 0.4545\n",
      "Epoch 7/200\n",
      " - 0s - loss: 2.3635 - acc: 0.4545\n",
      "Epoch 8/200\n",
      " - 0s - loss: 2.3469 - acc: 0.4545\n",
      "Epoch 9/200\n",
      " - 0s - loss: 2.3295 - acc: 0.4545\n",
      "Epoch 10/200\n",
      " - 0s - loss: 2.3113 - acc: 0.3636\n",
      "Epoch 11/200\n",
      " - 0s - loss: 2.2922 - acc: 0.3636\n",
      "Epoch 12/200\n",
      " - 0s - loss: 2.2721 - acc: 0.3636\n",
      "Epoch 13/200\n",
      " - 0s - loss: 2.2510 - acc: 0.3636\n",
      "Epoch 14/200\n",
      " - 0s - loss: 2.2290 - acc: 0.3636\n",
      "Epoch 15/200\n",
      " - 0s - loss: 2.2059 - acc: 0.3636\n",
      "Epoch 16/200\n",
      " - 0s - loss: 2.1820 - acc: 0.3636\n",
      "Epoch 17/200\n",
      " - 0s - loss: 2.1572 - acc: 0.3636\n",
      "Epoch 18/200\n",
      " - 0s - loss: 2.1318 - acc: 0.3636\n",
      "Epoch 19/200\n",
      " - 0s - loss: 2.1058 - acc: 0.3636\n",
      "Epoch 20/200\n",
      " - 0s - loss: 2.0795 - acc: 0.3636\n",
      "Epoch 21/200\n",
      " - 0s - loss: 2.0532 - acc: 0.3636\n",
      "Epoch 22/200\n",
      " - 0s - loss: 2.0273 - acc: 0.3636\n",
      "Epoch 23/200\n",
      " - 0s - loss: 2.0022 - acc: 0.3636\n",
      "Epoch 24/200\n",
      " - 0s - loss: 1.9783 - acc: 0.3636\n",
      "Epoch 25/200\n",
      " - 0s - loss: 1.9560 - acc: 0.3636\n",
      "Epoch 26/200\n",
      " - 0s - loss: 1.9357 - acc: 0.3636\n",
      "Epoch 27/200\n",
      " - 0s - loss: 1.9175 - acc: 0.3636\n",
      "Epoch 28/200\n",
      " - 0s - loss: 1.9014 - acc: 0.3636\n",
      "Epoch 29/200\n",
      " - 0s - loss: 1.8871 - acc: 0.3636\n",
      "Epoch 30/200\n",
      " - 0s - loss: 1.8741 - acc: 0.3636\n",
      "Epoch 31/200\n",
      " - 0s - loss: 1.8619 - acc: 0.3636\n",
      "Epoch 32/200\n",
      " - 0s - loss: 1.8497 - acc: 0.3636\n",
      "Epoch 33/200\n",
      " - 0s - loss: 1.8371 - acc: 0.3636\n",
      "Epoch 34/200\n",
      " - 0s - loss: 1.8236 - acc: 0.3636\n",
      "Epoch 35/200\n",
      " - 0s - loss: 1.8091 - acc: 0.3636\n",
      "Epoch 36/200\n",
      " - 0s - loss: 1.7936 - acc: 0.3636\n",
      "Epoch 37/200\n",
      " - 0s - loss: 1.7771 - acc: 0.3636\n",
      "Epoch 38/200\n",
      " - 0s - loss: 1.7602 - acc: 0.3636\n",
      "Epoch 39/200\n",
      " - 0s - loss: 1.7430 - acc: 0.3636\n",
      "Epoch 40/200\n",
      " - 0s - loss: 1.7258 - acc: 0.3636\n",
      "Epoch 41/200\n",
      " - 0s - loss: 1.7087 - acc: 0.3636\n",
      "Epoch 42/200\n",
      " - 0s - loss: 1.6920 - acc: 0.4545\n",
      "Epoch 43/200\n",
      " - 0s - loss: 1.6754 - acc: 0.4545\n",
      "Epoch 44/200\n",
      " - 0s - loss: 1.6591 - acc: 0.4545\n",
      "Epoch 45/200\n",
      " - 0s - loss: 1.6429 - acc: 0.4545\n",
      "Epoch 46/200\n",
      " - 0s - loss: 1.6265 - acc: 0.4545\n",
      "Epoch 47/200\n",
      " - 0s - loss: 1.6100 - acc: 0.4545\n",
      "Epoch 48/200\n",
      " - 0s - loss: 1.5932 - acc: 0.4545\n",
      "Epoch 49/200\n",
      " - 0s - loss: 1.5759 - acc: 0.4545\n",
      "Epoch 50/200\n",
      " - 0s - loss: 1.5582 - acc: 0.4545\n",
      "Epoch 51/200\n",
      " - 0s - loss: 1.5403 - acc: 0.4545\n",
      "Epoch 52/200\n",
      " - 0s - loss: 1.5218 - acc: 0.4545\n",
      "Epoch 53/200\n",
      " - 0s - loss: 1.5030 - acc: 0.4545\n",
      "Epoch 54/200\n",
      " - 0s - loss: 1.4839 - acc: 0.5455\n",
      "Epoch 55/200\n",
      " - 0s - loss: 1.4645 - acc: 0.5455\n",
      "Epoch 56/200\n",
      " - 0s - loss: 1.4451 - acc: 0.5455\n",
      "Epoch 57/200\n",
      " - 0s - loss: 1.4257 - acc: 0.5455\n",
      "Epoch 58/200\n",
      " - 0s - loss: 1.4062 - acc: 0.5455\n",
      "Epoch 59/200\n",
      " - 0s - loss: 1.3867 - acc: 0.5455\n",
      "Epoch 60/200\n",
      " - 0s - loss: 1.3672 - acc: 0.5455\n",
      "Epoch 61/200\n",
      " - 0s - loss: 1.3478 - acc: 0.5455\n",
      "Epoch 62/200\n",
      " - 0s - loss: 1.3286 - acc: 0.6364\n",
      "Epoch 63/200\n",
      " - 0s - loss: 1.3094 - acc: 0.6364\n",
      "Epoch 64/200\n",
      " - 0s - loss: 1.2905 - acc: 0.6364\n",
      "Epoch 65/200\n",
      " - 0s - loss: 1.2717 - acc: 0.6364\n",
      "Epoch 66/200\n",
      " - 0s - loss: 1.2532 - acc: 0.6364\n",
      "Epoch 67/200\n",
      " - 0s - loss: 1.2349 - acc: 0.6364\n",
      "Epoch 68/200\n",
      " - 0s - loss: 1.2169 - acc: 0.6364\n",
      "Epoch 69/200\n",
      " - 0s - loss: 1.1993 - acc: 0.6364\n",
      "Epoch 70/200\n",
      " - 0s - loss: 1.1819 - acc: 0.6364\n",
      "Epoch 71/200\n",
      " - 0s - loss: 1.1649 - acc: 0.6364\n",
      "Epoch 72/200\n",
      " - 0s - loss: 1.1483 - acc: 0.6364\n",
      "Epoch 73/200\n",
      " - 0s - loss: 1.1320 - acc: 0.6364\n",
      "Epoch 74/200\n",
      " - 0s - loss: 1.1160 - acc: 0.6364\n",
      "Epoch 75/200\n",
      " - 0s - loss: 1.1002 - acc: 0.6364\n",
      "Epoch 76/200\n",
      " - 0s - loss: 1.0848 - acc: 0.6364\n",
      "Epoch 77/200\n",
      " - 0s - loss: 1.0697 - acc: 0.6364\n",
      "Epoch 78/200\n",
      " - 0s - loss: 1.0547 - acc: 0.6364\n",
      "Epoch 79/200\n",
      " - 0s - loss: 1.0400 - acc: 0.6364\n",
      "Epoch 80/200\n",
      " - 0s - loss: 1.0256 - acc: 0.6364\n",
      "Epoch 81/200\n",
      " - 0s - loss: 1.0113 - acc: 0.6364\n",
      "Epoch 82/200\n",
      " - 0s - loss: 0.9972 - acc: 0.6364\n",
      "Epoch 83/200\n",
      " - 0s - loss: 0.9832 - acc: 0.6364\n",
      "Epoch 84/200\n",
      " - 0s - loss: 0.9694 - acc: 0.7273\n",
      "Epoch 85/200\n",
      " - 0s - loss: 0.9558 - acc: 0.7273\n",
      "Epoch 86/200\n",
      " - 0s - loss: 0.9423 - acc: 0.7273\n",
      "Epoch 87/200\n",
      " - 0s - loss: 0.9290 - acc: 0.7273\n",
      "Epoch 88/200\n",
      " - 0s - loss: 0.9158 - acc: 0.7273\n",
      "Epoch 89/200\n",
      " - 0s - loss: 0.9027 - acc: 0.7273\n",
      "Epoch 90/200\n",
      " - 0s - loss: 0.8897 - acc: 0.7273\n",
      "Epoch 91/200\n",
      " - 0s - loss: 0.8769 - acc: 0.7273\n",
      "Epoch 92/200\n",
      " - 0s - loss: 0.8642 - acc: 0.7273\n",
      "Epoch 93/200\n",
      " - 0s - loss: 0.8515 - acc: 0.7273\n",
      "Epoch 94/200\n",
      " - 0s - loss: 0.8390 - acc: 0.7273\n",
      "Epoch 95/200\n",
      " - 0s - loss: 0.8267 - acc: 0.7273\n",
      "Epoch 96/200\n",
      " - 0s - loss: 0.8144 - acc: 0.7273\n",
      "Epoch 97/200\n",
      " - 0s - loss: 0.8022 - acc: 0.7273\n",
      "Epoch 98/200\n",
      " - 0s - loss: 0.7902 - acc: 0.7273\n",
      "Epoch 99/200\n",
      " - 0s - loss: 0.7783 - acc: 0.7273\n",
      "Epoch 100/200\n",
      " - 0s - loss: 0.7666 - acc: 0.7273\n",
      "Epoch 101/200\n",
      " - 0s - loss: 0.7549 - acc: 0.7273\n",
      "Epoch 102/200\n",
      " - 0s - loss: 0.7434 - acc: 0.7273\n",
      "Epoch 103/200\n",
      " - 0s - loss: 0.7320 - acc: 0.8182\n",
      "Epoch 104/200\n",
      " - 0s - loss: 0.7207 - acc: 0.9091\n",
      "Epoch 105/200\n",
      " - 0s - loss: 0.7096 - acc: 0.9091\n",
      "Epoch 106/200\n",
      " - 0s - loss: 0.6985 - acc: 0.9091\n",
      "Epoch 107/200\n",
      " - 0s - loss: 0.6876 - acc: 0.9091\n",
      "Epoch 108/200\n",
      " - 0s - loss: 0.6768 - acc: 0.9091\n",
      "Epoch 109/200\n",
      " - 0s - loss: 0.6662 - acc: 0.9091\n",
      "Epoch 110/200\n",
      " - 0s - loss: 0.6556 - acc: 0.9091\n",
      "Epoch 111/200\n",
      " - 0s - loss: 0.6452 - acc: 0.9091\n",
      "Epoch 112/200\n",
      " - 0s - loss: 0.6349 - acc: 0.9091\n",
      "Epoch 113/200\n",
      " - 0s - loss: 0.6247 - acc: 0.9091\n",
      "Epoch 114/200\n",
      " - 0s - loss: 0.6147 - acc: 0.9091\n",
      "Epoch 115/200\n",
      " - 0s - loss: 0.6048 - acc: 0.9091\n",
      "Epoch 116/200\n",
      " - 0s - loss: 0.5950 - acc: 0.9091\n",
      "Epoch 117/200\n",
      " - 0s - loss: 0.5853 - acc: 0.9091\n",
      "Epoch 118/200\n",
      " - 0s - loss: 0.5757 - acc: 0.9091\n",
      "Epoch 119/200\n",
      " - 0s - loss: 0.5663 - acc: 0.9091\n",
      "Epoch 120/200\n",
      " - 0s - loss: 0.5570 - acc: 0.9091\n",
      "Epoch 121/200\n",
      " - 0s - loss: 0.5477 - acc: 0.9091\n",
      "Epoch 122/200\n",
      " - 0s - loss: 0.5386 - acc: 0.9091\n",
      "Epoch 123/200\n",
      " - 0s - loss: 0.5297 - acc: 0.9091\n",
      "Epoch 124/200\n",
      " - 0s - loss: 0.5208 - acc: 0.9091\n",
      "Epoch 125/200\n",
      " - 0s - loss: 0.5121 - acc: 0.9091\n",
      "Epoch 126/200\n",
      " - 0s - loss: 0.5034 - acc: 0.9091\n",
      "Epoch 127/200\n",
      " - 0s - loss: 0.4949 - acc: 0.9091\n",
      "Epoch 128/200\n",
      " - 0s - loss: 0.4865 - acc: 0.9091\n",
      "Epoch 129/200\n",
      " - 0s - loss: 0.4782 - acc: 0.9091\n",
      "Epoch 130/200\n",
      " - 0s - loss: 0.4701 - acc: 0.9091\n",
      "Epoch 131/200\n",
      " - 0s - loss: 0.4621 - acc: 0.9091\n",
      "Epoch 132/200\n",
      " - 0s - loss: 0.4542 - acc: 0.9091\n",
      "Epoch 133/200\n",
      " - 0s - loss: 0.4464 - acc: 0.9091\n",
      "Epoch 134/200\n",
      " - 0s - loss: 0.4387 - acc: 0.9091\n",
      "Epoch 135/200\n",
      " - 0s - loss: 0.4311 - acc: 0.9091\n",
      "Epoch 136/200\n",
      " - 0s - loss: 0.4237 - acc: 0.9091\n",
      "Epoch 137/200\n",
      " - 0s - loss: 0.4163 - acc: 0.9091\n",
      "Epoch 138/200\n",
      " - 0s - loss: 0.4091 - acc: 0.9091\n",
      "Epoch 139/200\n",
      " - 0s - loss: 0.4019 - acc: 0.9091\n",
      "Epoch 140/200\n",
      " - 0s - loss: 0.3949 - acc: 0.9091\n",
      "Epoch 141/200\n",
      " - 0s - loss: 0.3880 - acc: 0.9091\n",
      "Epoch 142/200\n",
      " - 0s - loss: 0.3812 - acc: 0.9091\n",
      "Epoch 143/200\n",
      " - 0s - loss: 0.3745 - acc: 0.9091\n",
      "Epoch 144/200\n",
      " - 0s - loss: 0.3680 - acc: 0.9091\n",
      "Epoch 145/200\n",
      " - 0s - loss: 0.3615 - acc: 0.9091\n",
      "Epoch 146/200\n",
      " - 0s - loss: 0.3551 - acc: 0.9091\n",
      "Epoch 147/200\n",
      " - 0s - loss: 0.3489 - acc: 0.9091\n",
      "Epoch 148/200\n",
      " - 0s - loss: 0.3427 - acc: 0.9091\n",
      "Epoch 149/200\n",
      " - 0s - loss: 0.3367 - acc: 0.9091\n",
      "Epoch 150/200\n",
      " - 0s - loss: 0.3308 - acc: 0.9091\n",
      "Epoch 151/200\n",
      " - 0s - loss: 0.3249 - acc: 0.9091\n",
      "Epoch 152/200\n",
      " - 0s - loss: 0.3192 - acc: 0.9091\n",
      "Epoch 153/200\n",
      " - 0s - loss: 0.3136 - acc: 0.9091\n",
      "Epoch 154/200\n",
      " - 0s - loss: 0.3081 - acc: 0.9091\n",
      "Epoch 155/200\n",
      " - 0s - loss: 0.3027 - acc: 0.9091\n",
      "Epoch 156/200\n",
      " - 0s - loss: 0.2974 - acc: 0.9091\n",
      "Epoch 157/200\n",
      " - 0s - loss: 0.2922 - acc: 0.9091\n",
      "Epoch 158/200\n",
      " - 0s - loss: 0.2871 - acc: 0.9091\n",
      "Epoch 159/200\n",
      " - 0s - loss: 0.2821 - acc: 0.9091\n",
      "Epoch 160/200\n",
      " - 0s - loss: 0.2772 - acc: 0.9091\n",
      "Epoch 161/200\n",
      " - 0s - loss: 0.2724 - acc: 0.9091\n",
      "Epoch 162/200\n",
      " - 0s - loss: 0.2677 - acc: 0.9091\n",
      "Epoch 163/200\n",
      " - 0s - loss: 0.2631 - acc: 0.9091\n",
      "Epoch 164/200\n",
      " - 0s - loss: 0.2585 - acc: 0.9091\n",
      "Epoch 165/200\n",
      " - 0s - loss: 0.2541 - acc: 0.9091\n",
      "Epoch 166/200\n",
      " - 0s - loss: 0.2498 - acc: 0.9091\n",
      "Epoch 167/200\n",
      " - 0s - loss: 0.2455 - acc: 0.9091\n",
      "Epoch 168/200\n",
      " - 0s - loss: 0.2413 - acc: 0.9091\n",
      "Epoch 169/200\n",
      " - 0s - loss: 0.2372 - acc: 0.9091\n",
      "Epoch 170/200\n",
      " - 0s - loss: 0.2332 - acc: 0.9091\n",
      "Epoch 171/200\n",
      " - 0s - loss: 0.2293 - acc: 0.9091\n",
      "Epoch 172/200\n",
      " - 0s - loss: 0.2255 - acc: 0.9091\n",
      "Epoch 173/200\n",
      " - 0s - loss: 0.2217 - acc: 0.9091\n",
      "Epoch 174/200\n",
      " - 0s - loss: 0.2180 - acc: 0.9091\n",
      "Epoch 175/200\n",
      " - 0s - loss: 0.2144 - acc: 0.9091\n",
      "Epoch 176/200\n",
      " - 0s - loss: 0.2109 - acc: 0.9091\n",
      "Epoch 177/200\n",
      " - 0s - loss: 0.2074 - acc: 0.9091\n",
      "Epoch 178/200\n",
      " - 0s - loss: 0.2040 - acc: 1.0000\n",
      "Epoch 179/200\n",
      " - 0s - loss: 0.2007 - acc: 1.0000\n",
      "Epoch 180/200\n",
      " - 0s - loss: 0.1974 - acc: 1.0000\n",
      "Epoch 181/200\n",
      " - 0s - loss: 0.1942 - acc: 1.0000\n",
      "Epoch 182/200\n",
      " - 0s - loss: 0.1910 - acc: 1.0000\n",
      "Epoch 183/200\n",
      " - 0s - loss: 0.1879 - acc: 1.0000\n",
      "Epoch 184/200\n",
      " - 0s - loss: 0.1849 - acc: 1.0000\n",
      "Epoch 185/200\n",
      " - 0s - loss: 0.1819 - acc: 1.0000\n",
      "Epoch 186/200\n",
      " - 0s - loss: 0.1790 - acc: 1.0000\n",
      "Epoch 187/200\n",
      " - 0s - loss: 0.1761 - acc: 1.0000\n",
      "Epoch 188/200\n",
      " - 0s - loss: 0.1733 - acc: 1.0000\n",
      "Epoch 189/200\n",
      " - 0s - loss: 0.1705 - acc: 1.0000\n",
      "Epoch 190/200\n",
      " - 0s - loss: 0.1678 - acc: 1.0000\n",
      "Epoch 191/200\n",
      " - 0s - loss: 0.1652 - acc: 1.0000\n",
      "Epoch 192/200\n",
      " - 0s - loss: 0.1625 - acc: 1.0000\n",
      "Epoch 193/200\n",
      " - 0s - loss: 0.1600 - acc: 1.0000\n",
      "Epoch 194/200\n",
      " - 0s - loss: 0.1574 - acc: 1.0000\n",
      "Epoch 195/200\n",
      " - 0s - loss: 0.1550 - acc: 1.0000\n",
      "Epoch 196/200\n",
      " - 0s - loss: 0.1525 - acc: 1.0000\n",
      "Epoch 197/200\n",
      " - 0s - loss: 0.1501 - acc: 1.0000\n",
      "Epoch 198/200\n",
      " - 0s - loss: 0.1478 - acc: 1.0000\n",
      "Epoch 199/200\n",
      " - 0s - loss: 0.1454 - acc: 1.0000\n",
      "Epoch 200/200\n",
      " - 0s - loss: 0.1432 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f209a36bf60>"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Embedding, Dense, SimpleRNN\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_len-1)) # 레이블을 분리하였으므로 이제 X의 길이는 5\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQ-s4Df5tZlw"
   },
   "source": [
    "문장을 생성하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rT1SYr4Hk_Vi"
   },
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
    "    sentence = ''\n",
    "    for _ in range(n): # n번 반복\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=5, padding='pre') # 데이터에 대한 패딩\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "    # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n",
    "        for word, index in t.word_index.items(): \n",
    "            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
    "                break # 해당 단어가 예측 단어이므로 break\n",
    "        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\n",
    "    # for문이므로 이 행동을 다시 반복\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1566364164297,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "S0SsFpYllDMH",
    "outputId": "59cdd4e6-b6d1-4e03-a1e5-9089255db84f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경마장에 있는 말이 뛰고 있다\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, '경마장에', 4))\n",
    "# '경마장에' 라는 단어 뒤에는 총 4개의 단어가 있으므로 4번 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 724,
     "status": "ok",
     "timestamp": 1566364169461,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "jCaelRjClF2k",
    "outputId": "2970c1c8-5d7b-47cb-f5f9-18b591c1ad9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그의 말이 법이다\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, '그의', 2)) # 2번 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 736,
     "status": "ok",
     "timestamp": 1566364175039,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "hWUXiYC-lHHl",
    "outputId": "ff93d58d-bcf3-4ebd-aebe-8bfcd1e2c6db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가는 말이 고와야 오는 말이 곱다\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, '가는', 5)) # 5번 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HGKXram1vIFH"
   },
   "source": [
    "# RNN한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRe3hUIrujkw"
   },
   "source": [
    " RNN은 출력 결과가 이전의 계산 결과에 의존한다는 것을 언급한 바 있습니다. 하지만, 앞서 배운 RNN은 비교적 짧은 시퀀스(sequence)에 대해서만 효과를 보이는 단점이 있습니다. 즉, RNN의 시점(time-step)이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1TJecmRuuG3"
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3WSzGDgWvi_3"
   },
   "source": [
    "RNN의  단점을 보완한 RNN의 일종을 장단기 메모리(Long Short-Term Memory)라고 하며, 줄여서 LSTM이라고 함\n",
    "LSTM은 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 불필요한 기억을 지우고, 기억해야할 것들을 정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Clt7RPN386BW"
   },
   "source": [
    "# LSTM을 이용하여 텍스트 생성\n",
    "데이터: 뉴욕 타임즈 기사의 제목"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 902,
     "status": "ok",
     "timestamp": 1566370867806,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "ksh-OTsNvkSh",
    "outputId": "76f856f1-1a0b-417c-bc07-6e9fc98029ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleID</th>\n",
       "      <th>articleWordCount</th>\n",
       "      <th>byline</th>\n",
       "      <th>documentType</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>newDesk</th>\n",
       "      <th>printPage</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>webURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5adf6684068401528a2aa69b</td>\n",
       "      <td>781</td>\n",
       "      <td>By JOHN BRANCH</td>\n",
       "      <td>article</td>\n",
       "      <td>Former N.F.L. Cheerleaders’ Settlement Offer: ...</td>\n",
       "      <td>['Workplace Hazards and Violations', 'Football...</td>\n",
       "      <td>68</td>\n",
       "      <td>Sports</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 17:16:49</td>\n",
       "      <td>Pro Football</td>\n",
       "      <td>“I understand that they could meet with us, pa...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/sports/foot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5adf653f068401528a2aa697</td>\n",
       "      <td>656</td>\n",
       "      <td>By LISA FRIEDMAN</td>\n",
       "      <td>article</td>\n",
       "      <td>E.P.A. to Unveil a New Rule. Its Effect: Less ...</td>\n",
       "      <td>['Environmental Protection Agency', 'Pruitt, S...</td>\n",
       "      <td>68</td>\n",
       "      <td>Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 17:11:21</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>The agency plans to publish a new regulation T...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/climate/epa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5adf4626068401528a2aa628</td>\n",
       "      <td>2427</td>\n",
       "      <td>By PETE WELLS</td>\n",
       "      <td>article</td>\n",
       "      <td>The New Noma, Explained</td>\n",
       "      <td>['Restaurants', 'Noma (Copenhagen, Restaurant)...</td>\n",
       "      <td>66</td>\n",
       "      <td>Dining</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:58:44</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>What’s it like to eat at the second incarnatio...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/dining/noma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5adf40d2068401528a2aa619</td>\n",
       "      <td>626</td>\n",
       "      <td>By JULIE HIRSCHFELD DAVIS and PETER BAKER</td>\n",
       "      <td>article</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>['Macron, Emmanuel (1977- )', 'Trump, Donald J...</td>\n",
       "      <td>68</td>\n",
       "      <td>Washington</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:35:57</td>\n",
       "      <td>Europe</td>\n",
       "      <td>President Trump welcomed President Emmanuel Ma...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/world/europ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5adf3d64068401528a2aa60f</td>\n",
       "      <td>815</td>\n",
       "      <td>By IAN AUSTEN and DAN BILEFSKY</td>\n",
       "      <td>article</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>['Toronto, Ontario, Attack (April, 2018)', 'Mu...</td>\n",
       "      <td>68</td>\n",
       "      <td>Foreign</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:21:21</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Alek Minassian, 25, a resident of Toronto’s Ri...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/world/canad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  articleID  ...                                             webURL\n",
       "0  5adf6684068401528a2aa69b  ...  https://www.nytimes.com/2018/04/24/sports/foot...\n",
       "1  5adf653f068401528a2aa697  ...  https://www.nytimes.com/2018/04/24/climate/epa...\n",
       "2  5adf4626068401528a2aa628  ...  https://www.nytimes.com/2018/04/24/dining/noma...\n",
       "3  5adf40d2068401528a2aa619  ...  https://www.nytimes.com/2018/04/24/world/europ...\n",
       "4  5adf3d64068401528a2aa60f  ...  https://www.nytimes.com/2018/04/24/world/canad...\n",
       "\n",
       "[5 rows x 15 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('/content/drive/My Drive/ArticlesApril2018.csv',encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 926,
     "status": "ok",
     "timestamp": 1566370879744,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "FPO6WVzZ9QTa",
    "outputId": "e62019ef-a247-45e2-adbf-7cbbf724c6b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['articleID', 'articleWordCount', 'byline', 'documentType', 'headline',\n",
      "       'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
      "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
      "      dtype='object')\n",
      "열의 개수:  15\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print('열의 개수: ',len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 608,
     "status": "ok",
     "timestamp": 1566370879748,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "1h-2RDtm-CL-",
    "outputId": "050cb04c-7afd-47d3-ab61-3e292e28f23a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['headline'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 598,
     "status": "ok",
     "timestamp": 1566370880121,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "t2ror2Y4-CzT",
    "outputId": "1b63a086-ea58-4c8a-8b60-03881cd279a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Former N.F.L. Cheerleaders’ Settlement Offer: $1 and a Meeting With Goodell',\n",
       " 'E.P.A. to Unveil a New Rule. Its Effect: Less Science in Policymaking.',\n",
       " 'The New Noma, Explained',\n",
       " 'Unknown',\n",
       " 'Unknown']"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline = [] # 리스트 선언\n",
    "headline.extend(list(df.headline.values)) # 헤드라인의 값들을 리스트로 저장\n",
    "headline[:5] # 상위 5개만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 637,
     "status": "ok",
     "timestamp": 1566370880523,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "-6aYvPvW-Fz7",
    "outputId": "a48205d4-dee7-44ad-8e67-5d698e9becec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1324"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(headline) # 현재 샘플의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 612,
     "status": "ok",
     "timestamp": 1566370880845,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "u-YxcHoV-KR7",
    "outputId": "1772aa00-0ae0-40f8-baf2-9f0d8f27da62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1214"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline = [n for n in headline if n != \"Unknown\"] # Unknown 값을 가진 샘플 제거\n",
    "len(headline) # 제거 후 샘플의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 706,
     "status": "ok",
     "timestamp": 1566370881286,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "ZDKrKeU2-NBa",
    "outputId": "90636c3d-2537-4b3f-cebc-cecb90329940"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Former N.F.L. Cheerleaders’ Settlement Offer: $1 and a Meeting With Goodell',\n",
       " 'E.P.A. to Unveil a New Rule. Its Effect: Less Science in Policymaking.',\n",
       " 'The New Noma, Explained',\n",
       " 'How a Bag of Texas Dirt  Became a Times Tradition',\n",
       " 'Is School a Place for Self-Expression?']"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nhm6mK1O_onN"
   },
   "source": [
    "데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 731,
     "status": "ok",
     "timestamp": 1566370881628,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "zmk9wxza-OzY",
    "outputId": "febf2bc8-1af8-4276-df0e-51fba4f60d7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['former nfl cheerleaders settlement offer 1 and a meeting with goodell',\n",
       " 'epa to unveil a new rule its effect less science in policymaking',\n",
       " 'the new noma explained',\n",
       " 'how a bag of texas dirt  became a times tradition',\n",
       " 'is school a place for selfexpression']"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "def repreprocessing(s):\n",
    "    s=s.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return ''.join(c for c in s if c not in punctuation).lower() # 구두점 제거와 동시에 소문자화\n",
    "\n",
    "text = [repreprocessing(x) for x in headline]\n",
    "text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jCpGC5hU_2qW"
   },
   "source": [
    "단어 집합 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 789,
     "status": "ok",
     "timestamp": 1566371335573,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "gKKckgVI-S-F",
    "outputId": "5f5cd661-a2ce-48f3-8a4e-246c05bc351d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단업 집합의 크기: 3494 \n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.text import Tokenizer\n",
    "\n",
    "t=Tokenizer()\n",
    "t.fit_on_texts(text)\n",
    "vocab_size=len(t.word_index)+1\n",
    "print('단업 집합의 크기: %d ' % vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EqkLubYqAhLu"
   },
   "source": [
    "LSTM 으로 문장생성 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 879,
     "status": "ok",
     "timestamp": 1566371431223,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "bvaFroRyAY5y",
    "outputId": "287c0fe5-c7ca-46d8-bf27-ab42c6a5d3a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[99, 269],\n",
       " [99, 269, 371],\n",
       " [99, 269, 371, 1115],\n",
       " [99, 269, 371, 1115, 582],\n",
       " [99, 269, 371, 1115, 582, 52],\n",
       " [99, 269, 371, 1115, 582, 52, 7],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2, 372],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2, 372, 10],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2, 372, 10, 1116],\n",
       " [100, 3]]"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences= list()\n",
    "\n",
    "for line in text: # 1,214 개의 샘플에 대해서 샘플을 1개씩 가져온다.\n",
    "    encoded = t.texts_to_sequences([line])[0] # 각 샘플에 대한 정수 인코딩\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "sequences[:11] # 11개의 샘플 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cci2d2k0Bfw9"
   },
   "source": [
    "어떤 정수가 어떤 단어를 의미하는지 알아보기 위해 인덱스로부터 단어를 찾는 index_to_word를 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1566371622756,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "pG7FN8DwAz-H",
    "outputId": "812d2fc4-27b9-47a1-9ba2-08b87c0fe7d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'offer'"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word={}\n",
    "for key, value in t.word_index.items(): # 인덱스를 단어로 바꾸기 위해 index_to_word를 생성\n",
    "    index_to_word[value] = key            #인덱스= 단어\n",
    "\n",
    "index_to_word[582]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2mrTcO1-B-ZO"
   },
   "source": [
    "패딩 작업을 수행하기 전에 가장 긴 샘플의 길이를 확인함\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 921,
     "status": "ok",
     "timestamp": 1566371766152,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "ox_vps1LBixy",
    "outputId": "b56b9511-9fc8-43bd-e4d4-05bc513a1822"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "max_len=max(len(l) for l in sequences)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LAEBj3rFCJzO"
   },
   "source": [
    "모든 샘플의 길이를 24로 맞춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 753,
     "status": "ok",
     "timestamp": 1566371879852,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "xfyhtGwVCFu1",
    "outputId": "4212a060-2d90-4ee3-aaa9-4a0c1ea39f89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0   99  269]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0   99  269  371]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0   99  269  371 1115]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
    "print(sequences[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pyhHviN6CvLH"
   },
   "source": [
    "x데이터와 y데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FxGJi7TyCc50"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 685,
     "status": "ok",
     "timestamp": 1566372004125,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "lOo8w-pWC6qY",
    "outputId": "99835350-82e3-4ffb-a1b2-17ee428e6527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0  99]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0  99 269]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  99 269 371]]\n"
     ]
    }
   ],
   "source": [
    "print(X[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 705,
     "status": "ok",
     "timestamp": 1566372075528,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "A1k9o5SmC_42",
    "outputId": "8801f98e-4cbe-4beb-9992-a02c3343525a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 269  371 1115]\n"
     ]
    }
   ],
   "source": [
    "print(y[:3]) # 레이블"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxwIeM9hDU1u"
   },
   "source": [
    "y 데이터에 대한 원핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "utNSEv1tDRUM"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 720,
     "status": "ok",
     "timestamp": 1566372203208,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "z_q44KRQDZza",
    "outputId": "53023efe-516d-475c-9e15-b1fcd2d6a7e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7803"
      ]
     },
     "execution_count": 93,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(y[2])  # 단어 수\n",
    "\n",
    "len(y)   # 샘플 수\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bh2jtDxnD0ju"
   },
   "source": [
    "모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1845173,
     "status": "ok",
     "timestamp": 1566374079148,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "0fPxmiW9DeZ5",
    "outputId": "2eec27a2-3a43-4f09-c871-8dc2d450f765"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 10s - loss: 7.6502 - acc: 0.0309\n",
      "Epoch 2/200\n",
      " - 9s - loss: 7.1222 - acc: 0.0301\n",
      "Epoch 3/200\n",
      " - 9s - loss: 6.9849 - acc: 0.0363\n",
      "Epoch 4/200\n",
      " - 9s - loss: 6.8623 - acc: 0.0423\n",
      "Epoch 5/200\n",
      " - 9s - loss: 6.7206 - acc: 0.0427\n",
      "Epoch 6/200\n",
      " - 9s - loss: 6.5645 - acc: 0.0459\n",
      "Epoch 7/200\n",
      " - 9s - loss: 6.3880 - acc: 0.0506\n",
      "Epoch 8/200\n",
      " - 9s - loss: 6.2036 - acc: 0.0536\n",
      "Epoch 9/200\n",
      " - 9s - loss: 6.0223 - acc: 0.0596\n",
      "Epoch 10/200\n",
      " - 9s - loss: 5.8451 - acc: 0.0637\n",
      "Epoch 11/200\n",
      " - 9s - loss: 5.6704 - acc: 0.0677\n",
      "Epoch 12/200\n",
      " - 9s - loss: 5.4978 - acc: 0.0724\n",
      "Epoch 13/200\n",
      " - 9s - loss: 5.3295 - acc: 0.0779\n",
      "Epoch 14/200\n",
      " - 9s - loss: 5.1673 - acc: 0.0843\n",
      "Epoch 15/200\n",
      " - 9s - loss: 5.0144 - acc: 0.0920\n",
      "Epoch 16/200\n",
      " - 9s - loss: 4.8664 - acc: 0.1035\n",
      "Epoch 17/200\n",
      " - 9s - loss: 4.7265 - acc: 0.1139\n",
      "Epoch 18/200\n",
      " - 9s - loss: 4.5915 - acc: 0.1266\n",
      "Epoch 19/200\n",
      " - 9s - loss: 4.4606 - acc: 0.1453\n",
      "Epoch 20/200\n",
      " - 9s - loss: 4.3354 - acc: 0.1601\n",
      "Epoch 21/200\n",
      " - 9s - loss: 4.2156 - acc: 0.1802\n",
      "Epoch 22/200\n",
      " - 9s - loss: 4.0969 - acc: 0.1998\n",
      "Epoch 23/200\n",
      " - 9s - loss: 3.9851 - acc: 0.2186\n",
      "Epoch 24/200\n",
      " - 9s - loss: 3.8739 - acc: 0.2364\n",
      "Epoch 25/200\n",
      " - 9s - loss: 3.7676 - acc: 0.2562\n",
      "Epoch 26/200\n",
      " - 9s - loss: 3.6651 - acc: 0.2708\n",
      "Epoch 27/200\n",
      " - 9s - loss: 3.5662 - acc: 0.2905\n",
      "Epoch 28/200\n",
      " - 9s - loss: 3.4734 - acc: 0.3021\n",
      "Epoch 29/200\n",
      " - 9s - loss: 3.3828 - acc: 0.3227\n",
      "Epoch 30/200\n",
      " - 9s - loss: 3.2954 - acc: 0.3351\n",
      "Epoch 31/200\n",
      " - 9s - loss: 3.2118 - acc: 0.3506\n",
      "Epoch 32/200\n",
      " - 9s - loss: 3.1319 - acc: 0.3638\n",
      "Epoch 33/200\n",
      " - 9s - loss: 3.0529 - acc: 0.3760\n",
      "Epoch 34/200\n",
      " - 9s - loss: 2.9754 - acc: 0.3916\n",
      "Epoch 35/200\n",
      " - 9s - loss: 2.9042 - acc: 0.4018\n",
      "Epoch 36/200\n",
      " - 9s - loss: 2.8377 - acc: 0.4156\n",
      "Epoch 37/200\n",
      " - 9s - loss: 2.7672 - acc: 0.4239\n",
      "Epoch 38/200\n",
      " - 9s - loss: 2.7042 - acc: 0.4403\n",
      "Epoch 39/200\n",
      " - 9s - loss: 2.6433 - acc: 0.4537\n",
      "Epoch 40/200\n",
      " - 9s - loss: 2.5855 - acc: 0.4615\n",
      "Epoch 41/200\n",
      " - 9s - loss: 2.5222 - acc: 0.4758\n",
      "Epoch 42/200\n",
      " - 9s - loss: 2.4653 - acc: 0.4905\n",
      "Epoch 43/200\n",
      " - 9s - loss: 2.4143 - acc: 0.5007\n",
      "Epoch 44/200\n",
      " - 9s - loss: 2.3564 - acc: 0.5034\n",
      "Epoch 45/200\n",
      " - 9s - loss: 2.3034 - acc: 0.5220\n",
      "Epoch 46/200\n",
      " - 9s - loss: 2.2549 - acc: 0.5289\n",
      "Epoch 47/200\n",
      " - 9s - loss: 2.2043 - acc: 0.5416\n",
      "Epoch 48/200\n",
      " - 9s - loss: 2.1567 - acc: 0.5541\n",
      "Epoch 49/200\n",
      " - 9s - loss: 2.1109 - acc: 0.5631\n",
      "Epoch 50/200\n",
      " - 9s - loss: 2.0606 - acc: 0.5749\n",
      "Epoch 51/200\n",
      " - 9s - loss: 2.0155 - acc: 0.5821\n",
      "Epoch 52/200\n",
      " - 9s - loss: 1.9706 - acc: 0.5918\n",
      "Epoch 53/200\n",
      " - 9s - loss: 1.9310 - acc: 0.6005\n",
      "Epoch 54/200\n",
      " - 9s - loss: 1.8853 - acc: 0.6149\n",
      "Epoch 55/200\n",
      " - 9s - loss: 1.8503 - acc: 0.6198\n",
      "Epoch 56/200\n",
      " - 9s - loss: 1.8061 - acc: 0.6281\n",
      "Epoch 57/200\n",
      " - 9s - loss: 1.7645 - acc: 0.6400\n",
      "Epoch 58/200\n",
      " - 9s - loss: 1.7281 - acc: 0.6467\n",
      "Epoch 59/200\n",
      " - 9s - loss: 1.6877 - acc: 0.6526\n",
      "Epoch 60/200\n",
      " - 9s - loss: 1.6517 - acc: 0.6595\n",
      "Epoch 61/200\n",
      " - 9s - loss: 1.6142 - acc: 0.6679\n",
      "Epoch 62/200\n",
      " - 9s - loss: 1.5801 - acc: 0.6770\n",
      "Epoch 63/200\n",
      " - 9s - loss: 1.5437 - acc: 0.6849\n",
      "Epoch 64/200\n",
      " - 9s - loss: 1.5079 - acc: 0.6920\n",
      "Epoch 65/200\n",
      " - 9s - loss: 1.4781 - acc: 0.6990\n",
      "Epoch 66/200\n",
      " - 9s - loss: 1.4410 - acc: 0.7060\n",
      "Epoch 67/200\n",
      " - 9s - loss: 1.4091 - acc: 0.7141\n",
      "Epoch 68/200\n",
      " - 9s - loss: 1.3786 - acc: 0.7199\n",
      "Epoch 69/200\n",
      " - 9s - loss: 1.3475 - acc: 0.7265\n",
      "Epoch 70/200\n",
      " - 9s - loss: 1.3153 - acc: 0.7357\n",
      "Epoch 71/200\n",
      " - 9s - loss: 1.2867 - acc: 0.7383\n",
      "Epoch 72/200\n",
      " - 9s - loss: 1.2577 - acc: 0.7425\n",
      "Epoch 73/200\n",
      " - 9s - loss: 1.2314 - acc: 0.7511\n",
      "Epoch 74/200\n",
      " - 9s - loss: 1.2031 - acc: 0.7571\n",
      "Epoch 75/200\n",
      " - 9s - loss: 1.1777 - acc: 0.7624\n",
      "Epoch 76/200\n",
      " - 9s - loss: 1.1516 - acc: 0.7677\n",
      "Epoch 77/200\n",
      " - 9s - loss: 1.1274 - acc: 0.7719\n",
      "Epoch 78/200\n",
      " - 9s - loss: 1.1000 - acc: 0.7779\n",
      "Epoch 79/200\n",
      " - 9s - loss: 1.0776 - acc: 0.7825\n",
      "Epoch 80/200\n",
      " - 9s - loss: 1.0542 - acc: 0.7891\n",
      "Epoch 81/200\n",
      " - 9s - loss: 1.0277 - acc: 0.7965\n",
      "Epoch 82/200\n",
      " - 9s - loss: 1.0083 - acc: 0.7969\n",
      "Epoch 83/200\n",
      " - 9s - loss: 0.9851 - acc: 0.8089\n",
      "Epoch 84/200\n",
      " - 9s - loss: 0.9622 - acc: 0.8085\n",
      "Epoch 85/200\n",
      " - 9s - loss: 0.9432 - acc: 0.8123\n",
      "Epoch 86/200\n",
      " - 9s - loss: 0.9217 - acc: 0.8174\n",
      "Epoch 87/200\n",
      " - 9s - loss: 0.9022 - acc: 0.8198\n",
      "Epoch 88/200\n",
      " - 9s - loss: 0.8846 - acc: 0.8228\n",
      "Epoch 89/200\n",
      " - 9s - loss: 0.8653 - acc: 0.8287\n",
      "Epoch 90/200\n",
      " - 9s - loss: 0.8482 - acc: 0.8304\n",
      "Epoch 91/200\n",
      " - 9s - loss: 0.8384 - acc: 0.8317\n",
      "Epoch 92/200\n",
      " - 9s - loss: 0.8101 - acc: 0.8389\n",
      "Epoch 93/200\n",
      " - 9s - loss: 0.7932 - acc: 0.8411\n",
      "Epoch 94/200\n",
      " - 9s - loss: 0.7801 - acc: 0.8424\n",
      "Epoch 95/200\n",
      " - 9s - loss: 0.7603 - acc: 0.8470\n",
      "Epoch 96/200\n",
      " - 9s - loss: 0.7449 - acc: 0.8512\n",
      "Epoch 97/200\n",
      " - 9s - loss: 0.7287 - acc: 0.8526\n",
      "Epoch 98/200\n",
      " - 9s - loss: 0.7146 - acc: 0.8540\n",
      "Epoch 99/200\n",
      " - 9s - loss: 0.6976 - acc: 0.8560\n",
      "Epoch 100/200\n",
      " - 9s - loss: 0.6875 - acc: 0.8595\n",
      "Epoch 101/200\n",
      " - 9s - loss: 0.6734 - acc: 0.8626\n",
      "Epoch 102/200\n",
      " - 9s - loss: 0.6598 - acc: 0.8657\n",
      "Epoch 103/200\n",
      " - 9s - loss: 0.6461 - acc: 0.8661\n",
      "Epoch 104/200\n",
      " - 9s - loss: 0.6391 - acc: 0.8685\n",
      "Epoch 105/200\n",
      " - 9s - loss: 0.6184 - acc: 0.8725\n",
      "Epoch 106/200\n",
      " - 9s - loss: 0.6086 - acc: 0.8763\n",
      "Epoch 107/200\n",
      " - 9s - loss: 0.5969 - acc: 0.8763\n",
      "Epoch 108/200\n",
      " - 9s - loss: 0.5865 - acc: 0.8784\n",
      "Epoch 109/200\n",
      " - 9s - loss: 0.5849 - acc: 0.8770\n",
      "Epoch 110/200\n",
      " - 9s - loss: 0.5794 - acc: 0.8808\n",
      "Epoch 111/200\n",
      " - 9s - loss: 0.5514 - acc: 0.8838\n",
      "Epoch 112/200\n",
      " - 9s - loss: 0.5413 - acc: 0.8859\n",
      "Epoch 113/200\n",
      " - 9s - loss: 0.5313 - acc: 0.8909\n",
      "Epoch 114/200\n",
      " - 9s - loss: 0.5238 - acc: 0.8886\n",
      "Epoch 115/200\n",
      " - 9s - loss: 0.5151 - acc: 0.8907\n",
      "Epoch 116/200\n",
      " - 9s - loss: 0.5146 - acc: 0.8902\n",
      "Epoch 117/200\n",
      " - 9s - loss: 0.5071 - acc: 0.8926\n",
      "Epoch 118/200\n",
      " - 9s - loss: 0.4876 - acc: 0.8950\n",
      "Epoch 119/200\n",
      " - 9s - loss: 0.4790 - acc: 0.8973\n",
      "Epoch 120/200\n",
      " - 9s - loss: 0.4732 - acc: 0.9004\n",
      "Epoch 121/200\n",
      " - 9s - loss: 0.4662 - acc: 0.8972\n",
      "Epoch 122/200\n",
      " - 9s - loss: 0.4582 - acc: 0.8990\n",
      "Epoch 123/200\n",
      " - 9s - loss: 0.4492 - acc: 0.9021\n",
      "Epoch 124/200\n",
      " - 9s - loss: 0.4442 - acc: 0.9032\n",
      "Epoch 125/200\n",
      " - 9s - loss: 0.4359 - acc: 0.9045\n",
      "Epoch 126/200\n",
      " - 9s - loss: 0.4313 - acc: 0.9045\n",
      "Epoch 127/200\n",
      " - 9s - loss: 0.4248 - acc: 0.9054\n",
      "Epoch 128/200\n",
      " - 9s - loss: 0.4232 - acc: 0.9058\n",
      "Epoch 129/200\n",
      " - 9s - loss: 0.4281 - acc: 0.9061\n",
      "Epoch 130/200\n",
      " - 9s - loss: 0.4117 - acc: 0.9082\n",
      "Epoch 131/200\n",
      " - 9s - loss: 0.4099 - acc: 0.9075\n",
      "Epoch 132/200\n",
      " - 9s - loss: 0.3986 - acc: 0.9090\n",
      "Epoch 133/200\n",
      " - 9s - loss: 0.3885 - acc: 0.9094\n",
      "Epoch 134/200\n",
      " - 9s - loss: 0.3864 - acc: 0.9084\n",
      "Epoch 135/200\n",
      " - 9s - loss: 0.3816 - acc: 0.9121\n",
      "Epoch 136/200\n",
      " - 9s - loss: 0.3768 - acc: 0.9089\n",
      "Epoch 137/200\n",
      " - 9s - loss: 0.3707 - acc: 0.9122\n",
      "Epoch 138/200\n",
      " - 9s - loss: 0.3694 - acc: 0.9129\n",
      "Epoch 139/200\n",
      " - 9s - loss: 0.3639 - acc: 0.9130\n",
      "Epoch 140/200\n",
      " - 9s - loss: 0.3627 - acc: 0.9094\n",
      "Epoch 141/200\n",
      " - 9s - loss: 0.3586 - acc: 0.9120\n",
      "Epoch 142/200\n",
      " - 9s - loss: 0.3525 - acc: 0.9117\n",
      "Epoch 143/200\n",
      " - 9s - loss: 0.3766 - acc: 0.9080\n",
      "Epoch 144/200\n",
      " - 9s - loss: 0.3738 - acc: 0.9102\n",
      "Epoch 145/200\n",
      " - 9s - loss: 0.3456 - acc: 0.9136\n",
      "Epoch 146/200\n",
      " - 9s - loss: 0.3384 - acc: 0.9149\n",
      "Epoch 147/200\n",
      " - 9s - loss: 0.3342 - acc: 0.9140\n",
      "Epoch 148/200\n",
      " - 9s - loss: 0.3306 - acc: 0.9164\n",
      "Epoch 149/200\n",
      " - 9s - loss: 0.3276 - acc: 0.9150\n",
      "Epoch 150/200\n",
      " - 9s - loss: 0.3280 - acc: 0.9149\n",
      "Epoch 151/200\n",
      " - 9s - loss: 0.3258 - acc: 0.9154\n",
      "Epoch 152/200\n",
      " - 9s - loss: 0.3241 - acc: 0.9152\n",
      "Epoch 153/200\n",
      " - 9s - loss: 0.3228 - acc: 0.9154\n",
      "Epoch 154/200\n",
      " - 9s - loss: 0.3185 - acc: 0.9155\n",
      "Epoch 155/200\n",
      " - 9s - loss: 0.3178 - acc: 0.9157\n",
      "Epoch 156/200\n",
      " - 9s - loss: 0.3229 - acc: 0.9143\n",
      "Epoch 157/200\n",
      " - 9s - loss: 0.3500 - acc: 0.9113\n",
      "Epoch 158/200\n",
      " - 9s - loss: 0.3169 - acc: 0.9163\n",
      "Epoch 159/200\n",
      " - 9s - loss: 0.3086 - acc: 0.9153\n",
      "Epoch 160/200\n",
      " - 9s - loss: 0.3044 - acc: 0.9166\n",
      "Epoch 161/200\n",
      " - 9s - loss: 0.3023 - acc: 0.9166\n",
      "Epoch 162/200\n",
      " - 9s - loss: 0.3026 - acc: 0.9175\n",
      "Epoch 163/200\n",
      " - 9s - loss: 0.3008 - acc: 0.9177\n",
      "Epoch 164/200\n",
      " - 9s - loss: 0.2972 - acc: 0.9164\n",
      "Epoch 165/200\n",
      " - 9s - loss: 0.2959 - acc: 0.9153\n",
      "Epoch 166/200\n",
      " - 9s - loss: 0.2971 - acc: 0.9163\n",
      "Epoch 167/200\n",
      " - 9s - loss: 0.2963 - acc: 0.9171\n",
      "Epoch 168/200\n",
      " - 9s - loss: 0.3279 - acc: 0.9114\n",
      "Epoch 169/200\n",
      " - 9s - loss: 0.3151 - acc: 0.9126\n",
      "Epoch 170/200\n",
      " - 9s - loss: 0.2957 - acc: 0.9163\n",
      "Epoch 171/200\n",
      " - 9s - loss: 0.2891 - acc: 0.9184\n",
      "Epoch 172/200\n",
      " - 9s - loss: 0.2878 - acc: 0.9181\n",
      "Epoch 173/200\n",
      " - 9s - loss: 0.2874 - acc: 0.9172\n",
      "Epoch 174/200\n",
      " - 9s - loss: 0.2868 - acc: 0.9177\n",
      "Epoch 175/200\n",
      " - 9s - loss: 0.2846 - acc: 0.9176\n",
      "Epoch 176/200\n",
      " - 9s - loss: 0.2856 - acc: 0.9162\n",
      "Epoch 177/200\n",
      " - 9s - loss: 0.2878 - acc: 0.9157\n",
      "Epoch 178/200\n",
      " - 9s - loss: 0.2850 - acc: 0.9168\n",
      "Epoch 179/200\n",
      " - 9s - loss: 0.2839 - acc: 0.9159\n",
      "Epoch 180/200\n",
      " - 9s - loss: 0.2876 - acc: 0.9144\n",
      "Epoch 181/200\n",
      " - 9s - loss: 0.2859 - acc: 0.9158\n",
      "Epoch 182/200\n",
      " - 9s - loss: 0.2823 - acc: 0.9155\n",
      "Epoch 183/200\n",
      " - 9s - loss: 0.2804 - acc: 0.9166\n",
      "Epoch 184/200\n",
      " - 9s - loss: 0.2796 - acc: 0.9170\n",
      "Epoch 185/200\n",
      " - 9s - loss: 0.2800 - acc: 0.9175\n",
      "Epoch 186/200\n",
      " - 9s - loss: 0.2805 - acc: 0.9175\n",
      "Epoch 187/200\n",
      " - 9s - loss: 0.2983 - acc: 0.9126\n",
      "Epoch 188/200\n",
      " - 9s - loss: 0.3328 - acc: 0.9082\n",
      "Epoch 189/200\n",
      " - 9s - loss: 0.2807 - acc: 0.9155\n",
      "Epoch 190/200\n",
      " - 9s - loss: 0.2755 - acc: 0.9161\n",
      "Epoch 191/200\n",
      " - 9s - loss: 0.2723 - acc: 0.9162\n",
      "Epoch 192/200\n",
      " - 9s - loss: 0.2720 - acc: 0.9164\n",
      "Epoch 193/200\n",
      " - 9s - loss: 0.2718 - acc: 0.9167\n",
      "Epoch 194/200\n",
      " - 9s - loss: 0.2714 - acc: 0.9166\n",
      "Epoch 195/200\n",
      " - 9s - loss: 0.2728 - acc: 0.9182\n",
      "Epoch 196/200\n",
      " - 9s - loss: 0.2711 - acc: 0.9168\n",
      "Epoch 197/200\n",
      " - 9s - loss: 0.2732 - acc: 0.9164\n",
      "Epoch 198/200\n",
      " - 9s - loss: 0.2828 - acc: 0.9140\n",
      "Epoch 199/200\n",
      " - 9s - loss: 0.2719 - acc: 0.9162\n",
      "Epoch 200/200\n",
      " - 9s - loss: 0.2699 - acc: 0.9159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2096cc20b8>"
      ]
     },
     "execution_count": 94,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Embedding, Dense, LSTM\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_len-1))\n",
    "# y데이터를 분리하였으므로 이제 X데이터의 길이는 기존 데이터의 길이 - 1\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3m5dS1AEKfw"
   },
   "source": [
    "LSTM 으로 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HQEtfYavD4Kz"
   },
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
    "    sentence = ''\n",
    "    for _ in range(n): # n번 반복\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=23, padding='pre') # 데이터에 대한 패딩\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "    # 입력한 X(현재 단어)에 대해서 y를 예측하고 y(예측한 단어)를 result에 저장.\n",
    "        for word, index in t.word_index.items(): \n",
    "            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
    "                break # 해당 단어가 예측 단어이므로 break\n",
    "        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\n",
    "    # for문이므로 이 행동을 다시 반복\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hNNd8RpxEfoI"
   },
   "source": [
    "확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1678836,
     "status": "ok",
     "timestamp": 1566374079464,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "1hNsDOZzESwo",
    "outputId": "527bf139-f94c-4386-e150-6115037e4065"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i cant jump ship from facebook yet help life hes president\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, 'i', 10))\n",
    "# 임의의 단어 'i'에 대해서 10개의 단어를 추가 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 680,
     "status": "ok",
     "timestamp": 1566374100437,
     "user": {
      "displayName": "xopuy2057@gmail.com",
      "photoUrl": "",
      "userId": "08173792587523637894"
     },
     "user_tz": -540
    },
    "id": "3Z0b3CbFEg2r",
    "outputId": "7f11d229-ec49-4953-b297-30903ed974c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to prevent a racist hoodie of ryan was in a\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, 'how', 10))\n",
    "# 임의의 단어 'how'에 대해서 10개의 단어를 추가 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zKBiLJwESRwm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "비정형 데이터(딥러닝).ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
