{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAM2klEQVR4nO3dX6wc5X3G8e9TG0IgRcZgkItRDRIioEoYekShVBWF0NI0glwkFSiq0gopN2kLTaQA7QWK1AsiVQm5qCJZkBRVlD8h0CArIrUcUNUbB/OnCWAIhrjgQrBJoaSJ1NTJrxczVk+dYzzHu3t21+/3Ix3tzuyu5l2PnjOze8bvk6pC0tHvl6Y9AEkrw7BLjTDsUiMMu9QIwy41wrBLjRgp7EmuSvJCkl1Jbh7XoCSNX4707+xJVgHfA64E9gCPA9dV1XPjG56kcVk9wmsvAnZV1csASe4FrgEOGfZTTjmlNm7cOMImJb2b3bt38+abb2apx0YJ++nAq4uW9wC/8W4v2LhxIzt27Bhhk5LezcLCwiEfG+Uz+1K/PX7hM0GSTyTZkWTHvn37RticpFGMEvY9wBmLljcArx38pKraXFULVbWwbt26ETYnaRSjhP1x4OwkZyY5FrgWeHg8w5I0bkf8mb2q9if5U+CbwCrgy1X17NhGJmmsRvmCjqr6BvCNMY1F0gR5BZ3UCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUiMOGPcmXk+xN8syidWuTbE3yYn970mSHKWlUQ47sfwdcddC6m4FtVXU2sK1fljTDDhv2qvpn4D8OWn0NcFd//y7gw2Mel6QxO9LP7KdV1esA/e2p4xuSpEmY+Bd0NsJIs+FIw/5GkvUA/e3eQz3RRhhpNhxp2B8GPt7f/zjw9fEMR9KkHLYkIsk9wGXAKUn2ALcCtwH3J7keeAX46CQHOQ7Jki22Opr9Qs3oSm56ihs/hMOGvaquO8RDV4x5LJImyCvopEYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYMaYQ5I8mjSXYmeTbJDf16W2GkOTLkyL4f+HRVnQtcDHwyyXnYCiPNlSGNMK9X1ZP9/R8BO4HTsRVGmivL+syeZCNwAbCdga0wlkRIs2Fw2JO8D/gacGNVvTP0dZZESLNhUNiTHEMX9Lur6sF+9eBWGEnTN+Tb+AB3Ajur6vOLHrIVRpojhy2JAC4F/gj4bpKn+3V/yRy2wkgtG9II8y/AobqTbIWR5oRX0EmNMOxSIwy71IghX9BJR2yaxcWWdP9/HtmlRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRgyZg+64JN9O8q99I8xn+/VnJtneN8Lcl+TYyQ9X0pEacmT/b+Dyqjof2ARcleRi4HPAF/pGmLeA6yc3TEmjGtIIU1X1X/3iMf1PAZcDD/TrbYSRZtzQeeNX9TPL7gW2Ai8Bb1fV/v4pe+gqoZZ6rY0w0gwYFPaq+llVbQI2ABcB5y71tEO81kYYaQYs69v4qnobeIyuzXVNkgPTWm0AXhvv0CSN05Bv49clWdPffy/wAbom10eBj/RPsxFGmnFDJpxcD9yVZBXdL4f7q2pLkueAe5P8NfAUXUWUpBk1pBHmO3Q1zQevf5nu87ukOeAVdFIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS42wsllHr2l2Nk+zq/oQPLJLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41YnDY++mkn0qypV+2EUaaI8s5st9AN9HkATbCSHNkaEnEBuAPgDv65WAjjDRXhh7Zbwc+A/y8Xz4ZG2GkuTJk3vgPAXur6onFq5d4qo0w0gwb8r/eLgWuTvJB4DjgRLoj/Zokq/uju40w0owb0uJ6S1VtqKqNwLXAt6rqY9gII82VUf7OfhPwqSS76D7D2wgjzbBlTV5RVY/RFTvaCCPNGa+gkxph2KVGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVG2M++EqbZ1T3NjvLpb16LeGSXGmHYpUYMOo1Pshv4EfAzYH9VLSRZC9wHbAR2A39YVW9NZpiSRrWcI/vvVNWmqlrol28GtvWNMNv6ZUkzapTT+GvommDARhhp5g0NewH/lOSJJJ/o151WVa8D9LenLvVCG2Gk2TD0T2+XVtVrSU4FtiZ5fugGqmozsBlgYWFhmn+Ekpo26MheVa/1t3uBh+imkH4jyXqA/nbvpAYpaXRDut5OSPLLB+4Dvws8AzxM1wQDNsJIM2/IafxpwENdSzOrgX+oqkeSPA7cn+R64BXgo5MbpqRRHTbsffPL+Uus/yFwxSQGJWn8vIJOaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapEYZdasSgsCdZk+SBJM8n2ZnkkiRrk2xN8mJ/e9KkByvpyA09sn8ReKSq3k83RdVObISR5sph56BLciLw28AfA1TVT4GfJrkGuKx/2l3AY8BNkxjk3Jtmb/G0Z+pPw33VM2bIkf0sYB/wlSRPJbmjn1LaRhhpjgwJ+2rgQuBLVXUB8GOWccpeVZuraqGqFtatW3eEw5Q0qiFh3wPsqart/fIDdOG3EUaaI4cNe1X9AHg1yTn9qiuA57ARRporQ4sd/wy4O8mxwMvAn9D9orARRpoTg8JeVU8DC0s8ZCOMNCe8gk5qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhw17knOSPL3o550kN1oSIc2XIXPQvVBVm6pqE/DrwE+Ah7AkQporyz2NvwJ4qar+DbiGrhyC/vbD4xyYpPFabtivBe7p7w8qiZA0GwaHvZ9Z9mrgq8vZgI0w0mxYzpH994Enq+qNfnlQSYSNMNJsWE7Yr+P/TuHBkghprgztZz8euBJ4cNHq24Ark7zYP3bb+IcnaVyGlkT8BDj5oHU/ZI5KIqqm3V3cqKn+s7vPF/MKOqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRQ6el+oskzyZ5Jsk9SY5LcmaS7X0jzH397LOSZtSQ+qfTgT8HFqrq14BVdPPHfw74Qt8I8xZw/SQHKmk0Q0/jVwPvTbIaOB54HbgceKB/3EYYacYN6Xr7d+BvgFfoQv6fwBPA21W1v3/aHuD0SQ1S0uiGnMafRNfrdibwK8AJdIURB1tyKk8bYaTZMOQ0/gPA96tqX1X9D93c8b8JrOlP6wE2AK8t9WIbYaTZMCTsrwAXJzk+Sejmin8OeBT4SP8cG2GkGTfkM/t2ui/ingS+279mM3AT8Kkku+gKJO6c4DgljWhoI8ytwK0HrX4ZuGjsI5I0EV5BJzXCsEuNMOxSIwy71IisZJVxkn3Aj4E3V2yjk3cKvp9ZdTS9Fxj2fn61qpa8oGVFww6QZEdVLazoRifI9zO7jqb3AqO/H0/jpUYYdqkR0wj75ilsc5J8P7PraHovMOL7WfHP7JKmw9N4qRErGvYkVyV5IcmuJDev5LZHleSMJI8m2dnPx3dDv35tkq39XHxb+///PzeSrEryVJIt/fLczi2YZE2SB5I83++nS+Z5/4x77scVC3uSVcDf0k18cR5wXZLzVmr7Y7Af+HRVnQtcDHyyH//NwLZ+Lr5t/fI8uQHYuWh5nucW/CLwSFW9Hzif7n3N5f6ZyNyPVbUiP8AlwDcXLd8C3LJS25/A+/k6cCXwArC+X7ceeGHaY1vGe9hAF4DLgS1A6C7aWL3UPpvlH+BE4Pv030MtWj+X+4dumrdXgbV0/zt1C/B7o+yflTyNPzD4A+Z23rokG4ELgO3AaVX1OkB/e+r0RrZstwOfAX7eL5/M/M4teBawD/hK/7HkjiQnMKf7pyYw9+NKhj1LrJu7PwUkeR/wNeDGqnpn2uM5Ukk+BOytqicWr17iqfOyj1YDFwJfqqoL6C7LnotT9qWMOvfjUlYy7HuAMxYtH3LeulmV5Bi6oN9dVQ/2q99Isr5/fD2wd1rjW6ZLgauT7AbupTuVv52BcwvOoD3AnupmVoJudqULmd/9M9Lcj0tZybA/Dpzdf5t4LN2XDQ+v4PZH0s+/dyews6o+v+ihh+nm4IM5mouvqm6pqg1VtZFuX3yrqj7GnM4tWFU/AF5Nck6/6sBciXO5f5jE3I8r/KXDB4HvAS8BfzXtL0GWOfbfojtl+g7wdP/zQbrPuduAF/vbtdMe6xG8t8uALf39s4BvA7uArwLvmfb4lvE+NgE7+n30j8BJ87x/gM8CzwPPAH8PvGeU/eMVdFIjvIJOaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapEf8LGTzOIZnNQIcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "\"\"\"\n",
    "Double Dueling DQN: Gridworld\n",
    "http://localhost:8888/notebooks/Desktop/%EA%B0%95%EC%9D%98%EA%B4%80%EB%A0%A8/Chap6.Double-Dueling-DQN.ipynb\n",
    "강화학습 첫걸음\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "에이전트는 파란색 사각형을 위, 아래, 왼쪽, 오른쪽으로 이동시킨다. \n",
    "목표는 빨간색 사각형 (-1의 보상)을 피하여 녹색 사각형 (+1의 보상)까지 도달하는 것이다. \n",
    "세가지 블록의 위치는 매 에피소드마다 랜덤하게 변하게 된다. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "from gridworld import gameEnv\n",
    "\n",
    "#========= 게임 환경 로딩 =========#\n",
    "env = gameEnv(partial=False,size=5)\n",
    "\n",
    "#========= 신경망 구현 =========#\n",
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        #신경망은 게임으로부터 하나의 프레임을 받아 이를 배열로 만든다 (flattening).\n",
    "        #배열의 크기를 재조절해주고 4개의 컨벌루션 레이어를 거치면서 처리해 준다.\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32)  #input(84*84*3 배열)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])      #input 데이터 모양을 바꿔줌\n",
    "        self.conv1 = slim.conv2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "            # 위 컨볼루션을 통과하면 사이즈는 20*20    shape(20,20,32)\n",
    "        \n",
    "        self.conv2 = slim.conv2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "               #  shape(9,9,64)\n",
    "        \n",
    "        self.conv3 = slim.conv2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "              #shape(7,7,64)\n",
    "        \n",
    "        self.conv4 = slim.conv2d( \\\n",
    "            inputs=self.conv3,num_outputs=h_size,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "                # shape(1,1,h_size)->   h_size=512 \n",
    "        \n",
    "        \n",
    "        #마지막 컨벌루션 레이어로부터의 출력값을 취한 후, 이를 어드밴티지 스트림과 값 스트림으로 분리한다. \n",
    "        self.streamAC,self.streamVC = tf.split(self.conv4,2,3)   #컬러를 나타내는 3을 기준으로 현재V ,A 두개로 나눠줌\n",
    "        self.streamA = slim.flatten(self.streamAC)                      #streamA(advantage) 와 streamV는 256 차원\n",
    "        self.streamV = slim.flatten(self.streamVC)\n",
    "       \n",
    "    #h_size/2 (256개)개 노드로 정규분포를 이용하여 AW, VW 가중치를 만듬\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2,env.actions]))  #env.actions의 초기값4을 표준편차로 해서 만듬\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2,1]))               # 1을 표준 편차로 해서 만듬\n",
    "        \n",
    "        #만들어진 AW와 conv4를 반으로 나눈 streamA 곱함(상수화 함)\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #최종 Q-값을 얻기 위해 어드밴티지 스트림과 값 스트림을 조합\n",
    "        #Advantage에서 Advantage의 평균을 뺀값+Value 값을 더해줌(가장 큰 Qout 값을 추출 )\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        \n",
    "        # 이것으로 행동을 고름\n",
    "        self.predict = tf.argmax(self.Qout,1)               \n",
    "        \n",
    "        #타겟 Q 값과 예측 Q 값 간의 제곱합 차를 취함으로써 비용을 구한다.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        \n",
    "        #상하좌우 방향을 원핫인코딩 해줌 4개로 \n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "        \n",
    "        #신경망에서 예상한 Qout 과 행동의 곱을 열기준으로 합하여 Q에 할당\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        #타겟 Q와 현재 Q의 차이를 제곱 으로 에러 구함\n",
    "        # 그에러를 평균으로 loss 을 구함 (평균 제곱 편차)\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        \n",
    "        # 아담을 이용하여  손실을 최저로 하는 쪽으로 으로 학습  \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= 경험 재생 =========#\n",
    "#다음 클래스는 경험과 샘플을 저장하고 랜덤하게 신경망을 학습시킨다 \n",
    "        \n",
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []                        # 버퍼 초기화, 버퍼사이즈 5만을 할당\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    #에피소드를 더할때 더할때 버퍼사이즈를 넘으면 앞에서부터 지우고 다시 넣는다\n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "    \n",
    "    #버퍼에서 랜덤으로 32 개의 값을 얻음 -> [32,5]로 만듬   (상태, 행동, 보상, 현재상태, 터미널)\n",
    "    def sample(self,size):\n",
    "        \n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])        \n",
    "\n",
    "#게임의 프레임의 사이즈를 조절해줌    21168=[84*84*3] 배열\n",
    "def processState(states):\n",
    "    return np.reshape(states,[21168])\n",
    "\n",
    "#아래 함수들은 1차 신경망의 파라미터와 함께 목표 신경망의 파라미터를 업데이트하게 해준다\n",
    "#학습 가능한 변수tfVars 와 매인 신경망에서 \n",
    "# target신경망을 업데이트할때 사용되는 비율 tau를 파라미터로 받음 \n",
    "def updateTargetGraph(tfVars,tau):\n",
    "    #tfVars의 절반의 앞은 main 신경망의 값, 나머지는 타겟 신경망 값   총 512개\n",
    "    total_vars = len(tfVars)    # 값은 12\n",
    "    op_holder = []\n",
    "   \n",
    "   \n",
    "    #여기서 앞의 절반에 tau를 곱했기 때문에 main 신경망의 가중치에 tau가 \n",
    "    #곱해지고,      \n",
    "    #나머지에 1-tau를 곱했기 때문에 타겟 신경망의 가중치에 1-tau가 곱해짐\n",
    "    #tau가 0.001 이기 때문에 99.999%로 타겟 신경망값이 유지 \n",
    "    #0.001% 로 main 신경망 값이 더해짐\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        \n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "# 타겟 신경망이 업데이트 \n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Sum:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"Sum_1:0\", shape=(?,), dtype=float32)\n",
      "[<tf.Variable 'Conv/weights:0' shape=(8, 8, 3, 32) dtype=float32_ref>, <tf.Variable 'Conv_1/weights:0' shape=(4, 4, 32, 64) dtype=float32_ref>, <tf.Variable 'Conv_2/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>, <tf.Variable 'Conv_3/weights:0' shape=(7, 7, 64, 512) dtype=float32_ref>, <tf.Variable 'Variable:0' shape=(256, 4) dtype=float32_ref>, <tf.Variable 'Variable_1:0' shape=(256, 1) dtype=float32_ref>, <tf.Variable 'Conv_4/weights:0' shape=(8, 8, 3, 32) dtype=float32_ref>, <tf.Variable 'Conv_5/weights:0' shape=(4, 4, 32, 64) dtype=float32_ref>, <tf.Variable 'Conv_6/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>, <tf.Variable 'Conv_7/weights:0' shape=(7, 7, 64, 512) dtype=float32_ref>, <tf.Variable 'Variable_2:0' shape=(256, 4) dtype=float32_ref>, <tf.Variable 'Variable_3:0' shape=(256, 1) dtype=float32_ref>]\n",
      "Saved Model\n",
      "500 1.2 0.0999999999999992\n",
      "1000 0.6 0.0999999999999992\n",
      "1500 0.2 0.0999999999999992\n",
      "2000 0.7 0.0999999999999992\n",
      "2500 0.6 0.0999999999999992\n",
      "3000 0.7 0.0999999999999992\n",
      "3500 0.0 0.0999999999999992\n",
      "4000 1.2 0.0999999999999992\n",
      "4500 0.5 0.0999999999999992\n",
      "5000 1.5 0.0999999999999992\n",
      "Percent of succesful episodes: 0.72%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f07ac0b4e0>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#========= 신경망 학습 =========#\n",
    "\n",
    "#모든 학습 파라미터를 설정한다            \n",
    "batch_size = 32 #각 학습 단계에서 사용할 경험의 수\n",
    "update_freq = 4 #학습 단계 기준의 업데이트 주기 \n",
    "y = .99 #타겟 Q-값에 대한 할인 계수\n",
    "startE = 1 #시작 시 랜덤 액션의 가능성\n",
    "endE = 0.1 #종료 시 랜덤 액션의 가능성\n",
    "anneling_steps = 100. #startE에서 endE로 줄어드는데 필요한 학습 단계 수\n",
    "num_episodes = 100 #신경망을 학습시키기 위한 게임 환경 에피소드의 수\n",
    "pre_train_steps = 100 #학습 시작 전 랜덤 액션의 단계 수\n",
    "max_epLength = 50 #허용되는 최대 에피소드 길이\n",
    "load_model = False #저장된 모델을 로딩할 지 여부\n",
    "path = \"./dqn\" #모델을 저장할 경로\n",
    "h_size = 512 #어드밴티지 스트림과 값 스트림으로 분리되기 전의 마지막 컨벌루션 레이어의 크기\n",
    "tau = 0.001 #타겟 신경망를  매인 신경망로 업데이트시켜 가는 비율\n",
    "\n",
    "\n",
    "# 그래프를 초기화한다\n",
    "tf.reset_default_graph()\n",
    "# Q 신경망 클래스를 통해 매인 신경망과 타겟 신경망 을 만든다\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "# 변수들을 초기화한다\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# saver를 만든다\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# 학습가능한 변수를 꺼낸다\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "# 타겟 신경망을 업데이트하기 위한 값을 만든다\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "# 경험 즉 에피소드를  저장할 장소\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#랜덤 액션이 감소하는 비율을 설정 (무작위 행위 확률을 설정)\n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps         #(1- 0.1/ 10000)\n",
    "\n",
    "#전체 보상과 에피소드 별 단계 수를 저장할 리스트를 생성, 총 스텝수 초기화 \n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#모델이 저장될 경로 생성\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# 텐서플로 세션을 연다\n",
    "with tf.Session() as sess:\n",
    "   #변수 초기화\n",
    "    sess.run(init)\n",
    "     # 모델을 불러올지 체크\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        \n",
    "        #모델을 불러옴\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    # 주요 신경망과 동일하게 타겟 신경망을 설정\n",
    "    updateTarget(targetOps,sess) \n",
    "    \n",
    "     # 에피소드 반복 시작 (학습)\n",
    "    for i in range(num_episodes):\n",
    "        \n",
    "        # 에피소드별 경험 버퍼를 초기화\n",
    "        episodeBuffer = experience_buffer()\n",
    "        \n",
    "        # 환경과 처음 상태을 초기화한다\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        \n",
    "        # 종료 여부\n",
    "        d = False\n",
    "        \n",
    "        # 보상\n",
    "        rAll = 0\n",
    "        \n",
    "        # 걸음\n",
    "        j = 0\n",
    "        \n",
    "        \n",
    "        #Q-신경망\n",
    "        # 만약 50 걸음을 초과하면  종료\n",
    "        while j < max_epLength: \n",
    "            j+=1\n",
    "           \n",
    "            # 총 스텝수가 학습 시작 전 랜덤 액션의 단계 수10000 보다 작거나\n",
    "            # 랜덤으로 뽑은수가 e 보다 작을 때 랜덤으로 행동\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                # 신경망을 통해 Q 값을 가져옴\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            \n",
    "             # 주어진 행동을 실행하고 다음 상태, 보상, 종료 여부를 가져옴\n",
    "            s1,r,d = env.step(a)\n",
    "            \n",
    "             # 상태를 다시 21168 차원으로 리사이즈\n",
    "            s1 = processState(s1)\n",
    "            \n",
    "            # 걸음수를 늘림\n",
    "            total_steps += 1\n",
    "            \n",
    "            # 버퍼에 현재 상태, 행동, 보상, 다음 상태, 종료 여부를 저장\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) \n",
    "            \n",
    "            \n",
    "            # 총스텝이 1만을 넘으면 시작\n",
    "            if total_steps > pre_train_steps:\n",
    "                \n",
    "                 # 무작위 확률 값을 줄임\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                # 총 걸음이 업데이트 수로 나누어 떨어지면 시작\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    \n",
    "                     # 경험으로부터 랜덤한 배치를 뽑음\n",
    "                    trainBatch = myBuffer.sample(batch_size) \n",
    "                    \n",
    "                   # target Q-value를 업데이트하는 Double-DQN을 수행 \n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    \n",
    "                    # 타겟 신경망에서 Q 값들을 얻음\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    \n",
    "                    # 종료 여부에 따라 가짜 라벨을 만듬 (비정상 종료는 1 , 정상종료는 0 )\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)  \n",
    "                    \n",
    "                # 타겟 신경망의 Q 값들 중에 주요 신경망에서 램덤으로 고른 32개의 Q 값들을 가져옴\n",
    "                    #(이부분이 doubleQ)     \n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    \n",
    "                     # 보상에 대한 더블 Q 값을 더해줌 , y는 할인율           #trainBatch 종료된 터미널 True  end_multiplier는 1 \n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    #타겟 값을 이용해 신경망 업데이트\n",
    "                    #행동들에 대해서 targetQ 값과의 차이를 통해 손실을 구하고 업데이트\n",
    "                    _ = sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n",
    "                    \n",
    "                    #주요 신경망과 타겟 신경망이 동일하도록 설정\n",
    "                    updateTarget(targetOps,sess) \n",
    "            \n",
    "            # 총 보상\n",
    "            rAll += r\n",
    "            \n",
    "            #상태를 바꿔줌\n",
    "            s = s1\n",
    "            \n",
    "            # 종료가 되면 멈춤\n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "        \n",
    "        # 이 에피소드로부터의 모든 경험을 저장\n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        \n",
    "        # 걸음을 저장\n",
    "        jList.append(j)\n",
    "        \n",
    "        # 보상을 저장\n",
    "        rList.append(rAll)\n",
    "        \n",
    "        #정기적으로 모델 저장\n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")\n",
    "        \n",
    "         # 최근 10개 에피소드의 평균 보상값을 나타냄\n",
    "        if len(rList) % 10 == 0:\n",
    "            print(total_steps,np.mean(rList[-10:]), e)\n",
    "    \n",
    "    # 모델을 저장한다.\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "    \n",
    "# 성공확률을 표시    \n",
    "print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")\n",
    "\n",
    "#========= 신경망 학습 확인하기 =========#\n",
    "#시간의 흐름에 따른 평균 보상\n",
    "\n",
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
