{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep Q-Network: Mountain Car에 적용한 DQN 예제\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, learning_rate, gamma, n_features, n_actions, epsilon, parameter_changing_pointer, memory_size):\n",
    "        self.learning_rate = learning_rate  #학습률\n",
    "        self.gamma = gamma            #할인율\n",
    "        self.n_features = n_features  #자동차의 위치(0)와 속도(1)\n",
    "        self.n_actions = n_actions    #왼쪽으로 밀기(0), 보류(1), 오른쪽으로 밀기(2) \n",
    "        self.epsilon = epsilon        #탐욕 정책 시 활용되는 탐욕의 초기 값       \n",
    "        self.batch_size = 100         #재생 메모리로부터 추출되는 표본의 크기\n",
    "        self.experience_counter = 0   #현재 재생 메모리에 저장된 표본의 수\n",
    "        self.experience_limit = memory_size  #재생 메모리의 최대 용량\n",
    "        self.replace_target_pointer = parameter_changing_pointer  #target network 갱신 기준 학습 단계\n",
    "        self.learning_counter = 0                                 #primary network의 학습 단계\n",
    "        self.memory = np.zeros([self.experience_limit,self.n_features*2+2])  #재생 메모리의 초기값  \n",
    "\n",
    "        self.build_networks() #primary network과 target network을 생성\n",
    "        p_params = tf.get_collection('primary_network_parameters')\n",
    "        t_params = tf.get_collection('target_network_parameters')\n",
    "        self.replacing_target_parameters = [tf.assign(t,p) for t,p in zip(t_params,p_params)]\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#========== DQN 모형의 기본 신경망 및 목표 신경망을 설정하는 단계 ============#\n",
    "\n",
    "    def build_networks(self):\n",
    "        hidden_units = 10\n",
    "#.....................................................................#\n",
    "        # Primary Network: 각 10개의 은닉노드를 갖는 2개의 은닉층 \n",
    "        self.s = tf.placeholder(tf.float32,[None,self.n_features])\n",
    "        self.qtarget = tf.placeholder(tf.float32,[None,self.n_actions])\n",
    "\n",
    "        with tf.variable_scope('primary_network'): #변수 볌위를 관리한다.\n",
    "            c = ['primary_network_parameters', \n",
    "\t\t\t\t\ttf.GraphKeys.GLOBAL_VARIABLES]\n",
    "            # first layer\n",
    "            with tf.variable_scope('layer1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, hidden_units],\n",
    "\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                b1 = tf.get_variable('b1', [1, hidden_units],\n",
    "\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1)\n",
    "\n",
    "            # second layer\n",
    "            with tf.variable_scope('layer2'):\n",
    "                w2 = tf.get_variable('w2', [hidden_units, self.n_actions],\n",
    "\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions],\n",
    "\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                self.qeval = tf.matmul(l1, w2) + b2\n",
    "\n",
    "        with tf.variable_scope('loss'):\n",
    "                self.loss = tf.reduce_mean(tf.squared_difference(\n",
    "\t\t\t\t\t\tself.qtarget, self.qeval))\n",
    "        with tf.variable_scope('optimiser'):\n",
    "                self.train = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "#.....................................................................#\n",
    "        # Target Network\n",
    "        self.st = tf.placeholder(tf.float32,[None,self.n_features])\n",
    "\n",
    "        with tf.variable_scope('target_network'):\n",
    "            c = ['target_network_parameters', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "            # first layer\n",
    "            with tf.variable_scope('layer1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, hidden_units],\n",
    "\t\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                b1 = tf.get_variable('b1', [1, hidden_units],\n",
    "\t\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                l1 = tf.nn.relu(tf.matmul(self.st, w1) + b1)\n",
    "\n",
    "            # second layer\n",
    "            with tf.variable_scope('layer2'):\n",
    "                w2 = tf.get_variable('w2', [hidden_units, self.n_actions],\n",
    "\t\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions],\n",
    "\t\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                self.qt = tf.matmul(l1, w2) + b2\n",
    "\n",
    "#-----------------------------------------------\n",
    "#기본 신경망의 학습 결과를 목표 신경망에 대입하기 위한 세션을 구성한다\n",
    "    def target_params_replaced(self):\n",
    "        self.sess.run(self.replacing_target_parameters)\n",
    "\n",
    "    def store_experience(self,obs,a,r,obs_):\n",
    "        index = self.experience_counter % self.experience_limit\n",
    "        self.memory[index,:] = np.hstack((obs,[a,r],obs_))\n",
    "        self.experience_counter+=1\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "#재생 메모리에 저장된 과거 경험을 설정한 배치 크기만큼 랜덤으로 추출하여 학습 데이터 집합으로 설정한다.\n",
    "    def fit(self):\n",
    "        # sample batch memory from all memory\n",
    "        if self.experience_counter < self.experience_limit:\n",
    "            indices = np.random.choice(self.experience_counter, size=self.batch_size)\n",
    "        else:\n",
    "            indices = np.random.choice(self.experience_limit, size=self.batch_size)\n",
    "\n",
    "        batch = self.memory[indices,:]\n",
    "        qt,qeval = self.sess.run([self.qt,self.qeval],\n",
    "\tfeed_dict={self.st:batch[:,-self.n_features:],self.s:batch[:,:self.n_features]})\n",
    "\n",
    "        qtarget = qeval.copy()    \n",
    "        batch_indices = np.arange(self.batch_size, dtype=np.int32)\n",
    "        actions = self.memory[indices,self.n_features].astype(int)\n",
    "        rewards = self.memory[indices,self.n_features+1]\n",
    "        qtarget[batch_indices,actions] = rewards + self.gamma * np.max(qt,axis=1)\n",
    "\n",
    "        _ = self.sess.run(self.train,feed_dict = {self.s:batch[:,:self.n_features],\n",
    "\t\t\t\t\t\t\tself.qtarget:qtarget})\n",
    "        if self.epsilon < 0.9:\n",
    "            self.epsilon += 0.0002\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "#학습 시 기본 신경망의 가중치를 가져와 목표 신경망의 가중치를 갱신한다.\n",
    "        if self.learning_counter % self.replace_target_pointer == 0:\n",
    "            self.target_params_replaced()\n",
    "            print(\"target parameters changed\")\n",
    "        self.learning_counter += 1\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "#탐욕 정책을 통해 행동을 선택하는 함수를 정의한다.\n",
    "    def epsilon_greedy(self,obs):\n",
    "        #epsilon greedy implementation to choose action\n",
    "        if np.random.uniform(low=0,high=1) < self.epsilon:\n",
    "            return np.argmax(self.sess.run(self.qeval,\n",
    "\t\t\t\t\tfeed_dict={self.s:obs[np.newaxis,:]}))\n",
    "        else:\n",
    "            return np.random.choice(self.n_actions)\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "#DQN 객체를 생성해 에이전트를 학습시키고 결과를 도출하는 단계\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    env = env.unwrapped\n",
    "    dqn = DQN(learning_rate=0.001, gamma=0.9, n_features=env.observation_space.shape[0], \t n_actions=env.action_space.n, epsilon=0.0, parameter_changing_pointer=500, \n",
    "\t memory_size=5000)\n",
    "\n",
    "    episodes = 10\n",
    "    total_steps = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        steps = 0\t\t\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            env.render()\n",
    "            action = dqn.epsilon_greedy(obs)\n",
    "            obs_,reward,terminate,_ = env.step(action)\n",
    "            reward = abs(obs_[0]+0.5)\n",
    "            dqn.store_experience(obs,action,reward,obs_)\n",
    "            if total_steps > 1000:\n",
    "                dqn.fit()\n",
    "            episode_reward+=reward\n",
    "            if terminate:\n",
    "                break\n",
    "            obs = obs_\n",
    "            total_steps+=1\n",
    "            steps+=1\n",
    "        print(\"Episode {} with Reward : {} at epsilon {} in steps {}\".\n",
    "\t\t\tformat(episode+1,episode_reward,dqn.epsilon,steps))\n",
    "\n",
    "    while True:  \n",
    "        env.render()\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
