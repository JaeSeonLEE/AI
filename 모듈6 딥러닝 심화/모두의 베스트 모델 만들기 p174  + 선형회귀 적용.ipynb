{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Keras","language":"python","name":"keras"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"widgets":{"state":{},"version":"1.1.2"},"colab":{"name":"모두의 베스트 모델 만들기 p174  + 선형회귀 적용.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"MB6Tbs1-23Sz","colab_type":"code","colab":{}},"source":["import tensorflow as tf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FNBKgcLz23S2","colab_type":"code","colab":{},"outputId":"294fa32f-5e16-45f9-ae35-c72530f33670"},"source":["\n","import tensorflow as tf\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","from pandas import DataFrame as df\n","import pandas as pd\n","import numpy\n","import matplotlib.pyplot as plt\n","import os\n","# seed 값 설정\n","seed = 0\n","numpy.random.seed(seed)\n","tf.set_random_seed(seed)\n","\n","# 데이터 입력\n","df_pre = pd.read_csv('C:/Users/Affinity/Desktop/lecture/006958-master/006958-master/deeplearning/dataset/wine.csv', header=None)\n","#df = df_pre.sample(frac=1)  #sample() 함수는 원본데이터에서 정해진 비율만큼 랜덤으로 뽑아옴/ frac=1 원본데이터 100퍼 불러옴\n","\n","\n","dataset = df_pre.values\n","X = dataset[:,0:12]\n","Y = dataset[:,12]\n","\n","# 모델 설정\n","model = Sequential()\n","model.add(Dense(30,  input_dim=12, activation='relu'))\n","model.add(Dense(12, activation='relu'))\n","model.add(Dense(8, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","#모델 컴파일\n","model.compile(loss='binary_crossentropy',\n","           optimizer='adam',\n","           metrics=['accuracy'])\n","\n","\n","\n","# 결과 출력\n","# print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))\n","\n","#모델 저장 폴더 설정\n","model_dir='./model/'\n","\n","if not os.path.exists(model_dir):\n","    os.mkdir(model_dir)\n","    \n","#모델 저장 조건 설정\n","\n","modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n","eraly_stopping_callback= EarlyStopping(monitor='val_loss',patience=10)\n","\n","\n","# 모델 실행 및 저장\n","model.fit(X, Y,validation_split=0.2 ,epochs=100, batch_size=100,callbacks=[eraly_stopping_callback])\n","\n","#결과 출력\n","print(\"\\n accuracy: %.4f\" % (model.evaluate(X,Y)[1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 5197 samples, validate on 1300 samples\n","Epoch 1/100\n","5197/5197 [==============================] - 1s 149us/step - loss: 0.5176 - acc: 0.8224 - val_loss: 0.2174 - val_acc: 0.9615\n","Epoch 2/100\n","5197/5197 [==============================] - 0s 38us/step - loss: 0.2822 - acc: 0.9123 - val_loss: 0.1655 - val_acc: 0.9646\n","Epoch 3/100\n","5197/5197 [==============================] - 0s 38us/step - loss: 0.2520 - acc: 0.9213 - val_loss: 0.1415 - val_acc: 0.9654\n","Epoch 4/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.2365 - acc: 0.9240 - val_loss: 0.1541 - val_acc: 0.9585\n","Epoch 5/100\n","5197/5197 [==============================] - 0s 39us/step - loss: 0.2246 - acc: 0.9255 - val_loss: 0.1438 - val_acc: 0.9600\n","Epoch 6/100\n","5197/5197 [==============================] - 0s 39us/step - loss: 0.2143 - acc: 0.9248 - val_loss: 0.1469 - val_acc: 0.9577\n","Epoch 7/100\n","5197/5197 [==============================] - 0s 39us/step - loss: 0.2057 - acc: 0.9263 - val_loss: 0.2011 - val_acc: 0.9408\n","Epoch 8/100\n","5197/5197 [==============================] - 0s 39us/step - loss: 0.1999 - acc: 0.9269 - val_loss: 0.1033 - val_acc: 0.9738\n","Epoch 9/100\n","5197/5197 [==============================] - 0s 39us/step - loss: 0.1936 - acc: 0.9296 - val_loss: 0.1306 - val_acc: 0.9692\n","Epoch 10/100\n","5197/5197 [==============================] - 0s 39us/step - loss: 0.1869 - acc: 0.9296 - val_loss: 0.1299 - val_acc: 0.9692\n","Epoch 11/100\n","5197/5197 [==============================] - 0s 39us/step - loss: 0.1802 - acc: 0.9332 - val_loss: 0.0771 - val_acc: 0.9815\n","Epoch 12/100\n","5197/5197 [==============================] - 0s 38us/step - loss: 0.1756 - acc: 0.9348 - val_loss: 0.0841 - val_acc: 0.9815\n","Epoch 13/100\n","5197/5197 [==============================] - 0s 39us/step - loss: 0.1713 - acc: 0.9371 - val_loss: 0.0822 - val_acc: 0.9831\n","Epoch 14/100\n","5197/5197 [==============================] - 0s 39us/step - loss: 0.1654 - acc: 0.9388 - val_loss: 0.1372 - val_acc: 0.9662\n","Epoch 15/100\n","5197/5197 [==============================] - 0s 38us/step - loss: 0.1635 - acc: 0.9398 - val_loss: 0.1688 - val_acc: 0.9515\n","Epoch 16/100\n","5197/5197 [==============================] - 0s 41us/step - loss: 0.1558 - acc: 0.9405 - val_loss: 0.0661 - val_acc: 0.9854\n","Epoch 17/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.1494 - acc: 0.9432 - val_loss: 0.0908 - val_acc: 0.9808\n","Epoch 18/100\n","5197/5197 [==============================] - 0s 39us/step - loss: 0.1478 - acc: 0.9450 - val_loss: 0.0697 - val_acc: 0.9838\n","Epoch 19/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.1452 - acc: 0.9448 - val_loss: 0.0927 - val_acc: 0.9777\n","Epoch 20/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.1371 - acc: 0.9486 - val_loss: 0.0613 - val_acc: 0.9869\n","Epoch 21/100\n","5197/5197 [==============================] - 0s 39us/step - loss: 0.1305 - acc: 0.9517 - val_loss: 0.0739 - val_acc: 0.9831\n","Epoch 22/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.1285 - acc: 0.9544 - val_loss: 0.0576 - val_acc: 0.9877\n","Epoch 23/100\n","5197/5197 [==============================] - 0s 39us/step - loss: 0.1234 - acc: 0.9571 - val_loss: 0.0564 - val_acc: 0.9869\n","Epoch 24/100\n","5197/5197 [==============================] - 0s 39us/step - loss: 0.1215 - acc: 0.9571 - val_loss: 0.0579 - val_acc: 0.9862\n","Epoch 25/100\n","5197/5197 [==============================] - 0s 39us/step - loss: 0.1188 - acc: 0.9588 - val_loss: 0.0448 - val_acc: 0.9900\n","Epoch 26/100\n","5197/5197 [==============================] - 0s 41us/step - loss: 0.1130 - acc: 0.9634 - val_loss: 0.0591 - val_acc: 0.9854\n","Epoch 27/100\n","5197/5197 [==============================] - 0s 41us/step - loss: 0.1117 - acc: 0.9640 - val_loss: 0.0615 - val_acc: 0.9862\n","Epoch 28/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.1089 - acc: 0.9638 - val_loss: 0.0703 - val_acc: 0.9831\n","Epoch 29/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.1094 - acc: 0.9644 - val_loss: 0.0625 - val_acc: 0.9846\n","Epoch 30/100\n","5197/5197 [==============================] - 0s 39us/step - loss: 0.1047 - acc: 0.9657 - val_loss: 0.0699 - val_acc: 0.9815\n","Epoch 31/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.1024 - acc: 0.9675 - val_loss: 0.0472 - val_acc: 0.9885\n","Epoch 32/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.0996 - acc: 0.9683 - val_loss: 0.0411 - val_acc: 0.9892\n","Epoch 33/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.0974 - acc: 0.9709 - val_loss: 0.0470 - val_acc: 0.9862\n","Epoch 34/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.0969 - acc: 0.9694 - val_loss: 0.0835 - val_acc: 0.9785\n","Epoch 35/100\n","5197/5197 [==============================] - 0s 41us/step - loss: 0.0978 - acc: 0.9694 - val_loss: 0.0365 - val_acc: 0.9915\n","Epoch 36/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.0964 - acc: 0.9704 - val_loss: 0.0984 - val_acc: 0.9723\n","Epoch 37/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.0914 - acc: 0.9725 - val_loss: 0.0724 - val_acc: 0.9792\n","Epoch 38/100\n","5197/5197 [==============================] - 0s 41us/step - loss: 0.0888 - acc: 0.9723 - val_loss: 0.0531 - val_acc: 0.9831\n","Epoch 39/100\n","5197/5197 [==============================] - 0s 43us/step - loss: 0.0905 - acc: 0.9704 - val_loss: 0.0705 - val_acc: 0.9800\n","Epoch 40/100\n","5197/5197 [==============================] - 0s 41us/step - loss: 0.0881 - acc: 0.9727 - val_loss: 0.0380 - val_acc: 0.9900\n","Epoch 41/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.0846 - acc: 0.9746 - val_loss: 0.0387 - val_acc: 0.9900\n","Epoch 42/100\n","5197/5197 [==============================] - 0s 41us/step - loss: 0.0862 - acc: 0.9740 - val_loss: 0.0451 - val_acc: 0.9885\n","Epoch 43/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.0843 - acc: 0.9750 - val_loss: 0.0493 - val_acc: 0.9854\n","Epoch 44/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.0838 - acc: 0.9742 - val_loss: 0.0227 - val_acc: 0.9962\n","Epoch 45/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.0843 - acc: 0.9754 - val_loss: 0.0452 - val_acc: 0.9877\n","Epoch 46/100\n","5197/5197 [==============================] - 0s 41us/step - loss: 0.0798 - acc: 0.9769 - val_loss: 0.0719 - val_acc: 0.9785\n","Epoch 47/100\n","5197/5197 [==============================] - 0s 43us/step - loss: 0.0779 - acc: 0.9773 - val_loss: 0.0384 - val_acc: 0.9877\n","Epoch 48/100\n","5197/5197 [==============================] - 0s 42us/step - loss: 0.0785 - acc: 0.9752 - val_loss: 0.0902 - val_acc: 0.9692\n","Epoch 49/100\n","5197/5197 [==============================] - 0s 41us/step - loss: 0.0790 - acc: 0.9750 - val_loss: 0.0339 - val_acc: 0.9885\n","Epoch 50/100\n","5197/5197 [==============================] - 0s 42us/step - loss: 0.0771 - acc: 0.9769 - val_loss: 0.0623 - val_acc: 0.9785\n","Epoch 51/100\n","5197/5197 [==============================] - 0s 42us/step - loss: 0.0783 - acc: 0.9754 - val_loss: 0.0330 - val_acc: 0.9915\n","Epoch 52/100\n","5197/5197 [==============================] - 0s 39us/step - loss: 0.0757 - acc: 0.9794 - val_loss: 0.0561 - val_acc: 0.9800\n","Epoch 53/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.0773 - acc: 0.9759 - val_loss: 0.0284 - val_acc: 0.9931\n","Epoch 54/100\n","5197/5197 [==============================] - 0s 40us/step - loss: 0.0803 - acc: 0.9761 - val_loss: 0.1018 - val_acc: 0.9654\n","6497/6497 [==============================] - 0s 38us/step\n","\n"," accuracy: 0.9724\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TddhDjXR23S4","colab_type":"code","colab":{}},"source":["# print((model.predict(X[1,:])))\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n6IGQoqO23S6","colab_type":"text"},"source":["# 선형회귀 적용하기\n","\n"]},{"cell_type":"code","metadata":{"id":"XCDHnNOF23S6","colab_type":"code","colab":{},"outputId":"947cce38-a9b7-4233-a66b-cf41d1f2a7fb"},"source":["import pandas as pd\n","\n","df=pd.read_csv(\"C:/Users/Affinity/Desktop/lecture/006958-master/006958-master/deeplearning/dataset/housing.csv\",delim_whitespace=True,header=None)\n","\n","print(df.info())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 506 entries, 0 to 505\n","Data columns (total 14 columns):\n","0     506 non-null float64\n","1     506 non-null float64\n","2     506 non-null float64\n","3     506 non-null int64\n","4     506 non-null float64\n","5     506 non-null float64\n","6     506 non-null float64\n","7     506 non-null float64\n","8     506 non-null int64\n","9     506 non-null float64\n","10    506 non-null float64\n","11    506 non-null float64\n","12    506 non-null float64\n","13    506 non-null float64\n","dtypes: float64(12), int64(2)\n","memory usage: 55.4 KB\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HDrj2Lzu23S8","colab_type":"code","colab":{},"outputId":"5f23c127-0c41-4b79-9aba-09f89b2569b8"},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","from sklearn.model_selection import train_test_split\n","\n","import numpy \n","import pandas as pd\n","import tensorflow as tf\n","\n","#seed 값 설정\n","seed=0\n","numpy.random.seed(seed)\n","tf.set_random_seed(seed)\n","\n","df=pd.read_csv('C:/Users/Affinity/Desktop/lecture/006958-master/006958-master/deeplearning/dataset/housing.csv',\n","               delim_whitespace=True,header=None)\n","\n","dataset=df.values\n","X=dataset[:,0:13]\n","Y=dataset[:,13]\n","\n","X_train, X_test, Y_train,Y_test =train_test_split(X,Y,test_size=0.3,random_state=seed)\n","\n","model=Sequential()\n","model.add(Dense(30,input_dim=13,activation='relu'))\n","model.add(Dense(6,activation='relu'))\n","model.add(Dense(1))\n","\n","model.compile(loss='mean_squared_error',optimizer='adam')\n","\n","model.fit(X_train,Y_train,epochs=200,batch_size=10)\n","\n","#예측값과 실제 값의 비교\n","\n","Y_prediction=model.predict(X_test).flatten()\n","\n","for i in range(10):\n","    label=Y_test[i]\n","    prediction=Y_prediction[i]\n","    print(\"실제가격: {:.3f}, 예상가격: {:3f}\".format(label,prediction))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/200\n","354/354 [==============================] - 1s 2ms/step - loss: 2189.1730\n","Epoch 2/200\n","354/354 [==============================] - 0s 304us/step - loss: 115.0839\n","Epoch 3/200\n","354/354 [==============================] - 0s 279us/step - loss: 77.4385\n","Epoch 4/200\n","354/354 [==============================] - 0s 285us/step - loss: 71.4009\n","Epoch 5/200\n","354/354 [==============================] - 0s 285us/step - loss: 69.7631\n","Epoch 6/200\n","354/354 [==============================] - 0s 282us/step - loss: 67.9045\n","Epoch 7/200\n","354/354 [==============================] - 0s 287us/step - loss: 63.8959\n","Epoch 8/200\n","354/354 [==============================] - 0s 279us/step - loss: 62.8500\n","Epoch 9/200\n","354/354 [==============================] - 0s 268us/step - loss: 61.0568\n","Epoch 10/200\n","354/354 [==============================] - 0s 279us/step - loss: 60.1992\n","Epoch 11/200\n","354/354 [==============================] - 0s 285us/step - loss: 57.8429\n","Epoch 12/200\n","354/354 [==============================] - 0s 276us/step - loss: 58.7804\n","Epoch 13/200\n","354/354 [==============================] - 0s 259us/step - loss: 57.4499\n","Epoch 14/200\n","354/354 [==============================] - 0s 268us/step - loss: 55.6314\n","Epoch 15/200\n","354/354 [==============================] - 0s 270us/step - loss: 55.0084\n","Epoch 16/200\n","354/354 [==============================] - 0s 265us/step - loss: 53.4578\n","Epoch 17/200\n","354/354 [==============================] - 0s 270us/step - loss: 53.2252\n","Epoch 18/200\n","354/354 [==============================] - 0s 268us/step - loss: 52.6546\n","Epoch 19/200\n","354/354 [==============================] - 0s 268us/step - loss: 52.6550\n","Epoch 20/200\n","354/354 [==============================] - 0s 273us/step - loss: 50.3159\n","Epoch 21/200\n","354/354 [==============================] - 0s 270us/step - loss: 49.4774\n","Epoch 22/200\n","354/354 [==============================] - 0s 265us/step - loss: 49.0772\n","Epoch 23/200\n","354/354 [==============================] - 0s 270us/step - loss: 48.2556\n","Epoch 24/200\n","354/354 [==============================] - 0s 265us/step - loss: 46.5846\n","Epoch 25/200\n","354/354 [==============================] - 0s 279us/step - loss: 46.2973\n","Epoch 26/200\n","354/354 [==============================] - 0s 276us/step - loss: 45.7144\n","Epoch 27/200\n","354/354 [==============================] - 0s 270us/step - loss: 45.0218\n","Epoch 28/200\n","354/354 [==============================] - 0s 282us/step - loss: 43.5277\n","Epoch 29/200\n","354/354 [==============================] - 0s 265us/step - loss: 42.5835\n","Epoch 30/200\n","354/354 [==============================] - 0s 265us/step - loss: 41.5812\n","Epoch 31/200\n","354/354 [==============================] - 0s 279us/step - loss: 42.6652\n","Epoch 32/200\n","354/354 [==============================] - 0s 273us/step - loss: 43.2659\n","Epoch 33/200\n","354/354 [==============================] - 0s 270us/step - loss: 40.5995\n","Epoch 34/200\n","354/354 [==============================] - 0s 273us/step - loss: 41.3657\n","Epoch 35/200\n","354/354 [==============================] - 0s 276us/step - loss: 38.9875\n","Epoch 36/200\n","354/354 [==============================] - 0s 276us/step - loss: 38.7946\n","Epoch 37/200\n","354/354 [==============================] - 0s 276us/step - loss: 37.8362\n","Epoch 38/200\n","354/354 [==============================] - 0s 270us/step - loss: 39.6075\n","Epoch 39/200\n","354/354 [==============================] - 0s 262us/step - loss: 38.0081\n","Epoch 40/200\n","354/354 [==============================] - 0s 282us/step - loss: 38.6050\n","Epoch 41/200\n","354/354 [==============================] - 0s 270us/step - loss: 38.0922\n","Epoch 42/200\n","354/354 [==============================] - 0s 270us/step - loss: 35.7032\n","Epoch 43/200\n","354/354 [==============================] - 0s 270us/step - loss: 35.3246\n","Epoch 44/200\n","354/354 [==============================] - 0s 270us/step - loss: 35.1230\n","Epoch 45/200\n","354/354 [==============================] - 0s 287us/step - loss: 35.7028\n","Epoch 46/200\n","354/354 [==============================] - 0s 276us/step - loss: 34.6068\n","Epoch 47/200\n","354/354 [==============================] - 0s 273us/step - loss: 35.7248\n","Epoch 48/200\n","354/354 [==============================] - 0s 279us/step - loss: 35.2588\n","Epoch 49/200\n","354/354 [==============================] - 0s 273us/step - loss: 34.7024\n","Epoch 50/200\n","354/354 [==============================] - 0s 273us/step - loss: 34.6634\n","Epoch 51/200\n","354/354 [==============================] - 0s 285us/step - loss: 32.8713\n","Epoch 52/200\n","354/354 [==============================] - 0s 273us/step - loss: 32.9891\n","Epoch 53/200\n","354/354 [==============================] - 0s 265us/step - loss: 33.7879\n","Epoch 54/200\n","354/354 [==============================] - 0s 276us/step - loss: 33.1272\n","Epoch 55/200\n","354/354 [==============================] - 0s 282us/step - loss: 32.3414\n","Epoch 56/200\n","354/354 [==============================] - 0s 276us/step - loss: 31.4458\n","Epoch 57/200\n","354/354 [==============================] - 0s 293us/step - loss: 32.1178\n","Epoch 58/200\n","354/354 [==============================] - 0s 290us/step - loss: 31.6359\n","Epoch 59/200\n","354/354 [==============================] - 0s 282us/step - loss: 29.4711\n","Epoch 60/200\n","354/354 [==============================] - 0s 285us/step - loss: 29.7323\n","Epoch 61/200\n","354/354 [==============================] - 0s 293us/step - loss: 29.9915\n","Epoch 62/200\n","354/354 [==============================] - 0s 293us/step - loss: 30.3253\n","Epoch 63/200\n","354/354 [==============================] - 0s 299us/step - loss: 29.9273\n","Epoch 64/200\n","354/354 [==============================] - 0s 276us/step - loss: 29.4000\n","Epoch 65/200\n","354/354 [==============================] - 0s 282us/step - loss: 28.2698\n","Epoch 66/200\n","354/354 [==============================] - 0s 273us/step - loss: 28.8221\n","Epoch 67/200\n","354/354 [==============================] - 0s 276us/step - loss: 29.4170\n","Epoch 68/200\n","354/354 [==============================] - 0s 279us/step - loss: 27.4769\n","Epoch 69/200\n","354/354 [==============================] - 0s 282us/step - loss: 27.0949\n","Epoch 70/200\n","354/354 [==============================] - 0s 287us/step - loss: 26.6032\n","Epoch 71/200\n","354/354 [==============================] - 0s 270us/step - loss: 27.5161\n","Epoch 72/200\n","354/354 [==============================] - 0s 293us/step - loss: 28.1887\n","Epoch 73/200\n","354/354 [==============================] - 0s 265us/step - loss: 27.2687\n","Epoch 74/200\n","354/354 [==============================] - 0s 270us/step - loss: 26.9689\n","Epoch 75/200\n","354/354 [==============================] - 0s 279us/step - loss: 25.4722\n","Epoch 76/200\n","354/354 [==============================] - 0s 265us/step - loss: 24.2271\n","Epoch 77/200\n","354/354 [==============================] - 0s 287us/step - loss: 27.4854\n","Epoch 78/200\n","354/354 [==============================] - 0s 285us/step - loss: 24.5350\n","Epoch 79/200\n","354/354 [==============================] - 0s 273us/step - loss: 25.8633\n","Epoch 80/200\n","354/354 [==============================] - 0s 279us/step - loss: 25.4080\n","Epoch 81/200\n","354/354 [==============================] - 0s 268us/step - loss: 24.5142\n","Epoch 82/200\n","354/354 [==============================] - 0s 273us/step - loss: 23.6247\n","Epoch 83/200\n","354/354 [==============================] - 0s 290us/step - loss: 24.7268\n","Epoch 84/200\n","354/354 [==============================] - 0s 276us/step - loss: 24.1000\n","Epoch 85/200\n","354/354 [==============================] - 0s 282us/step - loss: 23.1300\n","Epoch 86/200\n","354/354 [==============================] - 0s 273us/step - loss: 23.1922\n","Epoch 87/200\n","354/354 [==============================] - 0s 282us/step - loss: 23.4276\n","Epoch 88/200\n","354/354 [==============================] - 0s 287us/step - loss: 24.4135\n","Epoch 89/200\n","354/354 [==============================] - 0s 270us/step - loss: 22.2244\n","Epoch 90/200\n","354/354 [==============================] - 0s 282us/step - loss: 22.7088\n","Epoch 91/200\n","354/354 [==============================] - 0s 282us/step - loss: 20.3189\n","Epoch 92/200\n","354/354 [==============================] - 0s 299us/step - loss: 21.8912\n","Epoch 93/200\n","354/354 [==============================] - 0s 268us/step - loss: 22.0981\n","Epoch 94/200\n","354/354 [==============================] - 0s 282us/step - loss: 21.3487\n","Epoch 95/200\n","354/354 [==============================] - 0s 273us/step - loss: 20.8156\n","Epoch 96/200\n"],"name":"stdout"},{"output_type":"stream","text":["354/354 [==============================] - 0s 296us/step - loss: 21.5591\n","Epoch 97/200\n","354/354 [==============================] - 0s 262us/step - loss: 21.9414\n","Epoch 98/200\n","354/354 [==============================] - 0s 270us/step - loss: 21.3639\n","Epoch 99/200\n","354/354 [==============================] - 0s 270us/step - loss: 20.4372\n","Epoch 100/200\n","354/354 [==============================] - 0s 268us/step - loss: 20.2442\n","Epoch 101/200\n","354/354 [==============================] - 0s 273us/step - loss: 26.1313\n","Epoch 102/200\n","354/354 [==============================] - 0s 268us/step - loss: 20.9311\n","Epoch 103/200\n","354/354 [==============================] - 0s 270us/step - loss: 20.9236\n","Epoch 104/200\n","354/354 [==============================] - 0s 273us/step - loss: 19.3695\n","Epoch 105/200\n","354/354 [==============================] - 0s 273us/step - loss: 21.6179\n","Epoch 106/200\n","354/354 [==============================] - 0s 270us/step - loss: 19.9599\n","Epoch 107/200\n","354/354 [==============================] - 0s 262us/step - loss: 18.9812\n","Epoch 108/200\n","354/354 [==============================] - 0s 268us/step - loss: 19.1473\n","Epoch 109/200\n","354/354 [==============================] - 0s 268us/step - loss: 19.0718\n","Epoch 110/200\n","354/354 [==============================] - 0s 265us/step - loss: 22.1908\n","Epoch 111/200\n","354/354 [==============================] - 0s 265us/step - loss: 18.1210\n","Epoch 112/200\n","354/354 [==============================] - 0s 265us/step - loss: 18.1897\n","Epoch 113/200\n","354/354 [==============================] - 0s 265us/step - loss: 19.8739\n","Epoch 114/200\n","354/354 [==============================] - 0s 262us/step - loss: 17.8975\n","Epoch 115/200\n","354/354 [==============================] - 0s 270us/step - loss: 18.4602\n","Epoch 116/200\n","354/354 [==============================] - 0s 282us/step - loss: 19.9784\n","Epoch 117/200\n","354/354 [==============================] - 0s 276us/step - loss: 20.3009\n","Epoch 118/200\n","354/354 [==============================] - 0s 256us/step - loss: 19.2855\n","Epoch 119/200\n","354/354 [==============================] - 0s 270us/step - loss: 19.0990\n","Epoch 120/200\n","354/354 [==============================] - 0s 265us/step - loss: 18.5731\n","Epoch 121/200\n","354/354 [==============================] - 0s 262us/step - loss: 18.3733\n","Epoch 122/200\n","354/354 [==============================] - 0s 270us/step - loss: 19.3084\n","Epoch 123/200\n","354/354 [==============================] - 0s 276us/step - loss: 18.0269\n","Epoch 124/200\n","354/354 [==============================] - 0s 268us/step - loss: 18.6717\n","Epoch 125/200\n","354/354 [==============================] - 0s 273us/step - loss: 20.6010\n","Epoch 126/200\n","354/354 [==============================] - 0s 268us/step - loss: 17.3931\n","Epoch 127/200\n","354/354 [==============================] - 0s 265us/step - loss: 18.0756\n","Epoch 128/200\n","354/354 [==============================] - 0s 268us/step - loss: 16.8261\n","Epoch 129/200\n","354/354 [==============================] - 0s 270us/step - loss: 17.6323\n","Epoch 130/200\n","354/354 [==============================] - 0s 268us/step - loss: 16.8087\n","Epoch 131/200\n","354/354 [==============================] - 0s 276us/step - loss: 16.9526\n","Epoch 132/200\n","354/354 [==============================] - 0s 262us/step - loss: 16.1726\n","Epoch 133/200\n","354/354 [==============================] - 0s 270us/step - loss: 16.0873\n","Epoch 134/200\n","354/354 [==============================] - 0s 259us/step - loss: 19.8876\n","Epoch 135/200\n","354/354 [==============================] - 0s 270us/step - loss: 17.0819\n","Epoch 136/200\n","354/354 [==============================] - 0s 259us/step - loss: 16.5381\n","Epoch 137/200\n","354/354 [==============================] - 0s 287us/step - loss: 16.8166\n","Epoch 138/200\n","354/354 [==============================] - 0s 330us/step - loss: 18.3508\n","Epoch 139/200\n","354/354 [==============================] - 0s 332us/step - loss: 17.3035\n","Epoch 140/200\n","354/354 [==============================] - 0s 324us/step - loss: 15.2240\n","Epoch 141/200\n","354/354 [==============================] - 0s 332us/step - loss: 16.9960\n","Epoch 142/200\n","354/354 [==============================] - 0s 318us/step - loss: 16.4756\n","Epoch 143/200\n","354/354 [==============================] - 0s 296us/step - loss: 16.3486\n","Epoch 144/200\n","354/354 [==============================] - 0s 287us/step - loss: 15.5107\n","Epoch 145/200\n","354/354 [==============================] - 0s 279us/step - loss: 16.8076\n","Epoch 146/200\n","354/354 [==============================] - 0s 287us/step - loss: 15.1658\n","Epoch 147/200\n","354/354 [==============================] - 0s 284us/step - loss: 14.7456\n","Epoch 148/200\n","354/354 [==============================] - 0s 273us/step - loss: 16.0404\n","Epoch 149/200\n","354/354 [==============================] - 0s 290us/step - loss: 15.3616\n","Epoch 150/200\n","354/354 [==============================] - 0s 344us/step - loss: 15.8106\n","Epoch 151/200\n","354/354 [==============================] - 0s 330us/step - loss: 14.8471\n","Epoch 152/200\n","354/354 [==============================] - 0s 327us/step - loss: 16.4725\n","Epoch 153/200\n","354/354 [==============================] - 0s 324us/step - loss: 15.8134\n","Epoch 154/200\n","354/354 [==============================] - 0s 321us/step - loss: 14.9368\n","Epoch 155/200\n","354/354 [==============================] - 0s 335us/step - loss: 15.6296\n","Epoch 156/200\n","354/354 [==============================] - 0s 313us/step - loss: 15.6575\n","Epoch 157/200\n","354/354 [==============================] - 0s 299us/step - loss: 16.3839\n","Epoch 158/200\n","354/354 [==============================] - 0s 290us/step - loss: 15.0205\n","Epoch 159/200\n","354/354 [==============================] - 0s 287us/step - loss: 14.6103\n","Epoch 160/200\n","354/354 [==============================] - 0s 304us/step - loss: 16.2845\n","Epoch 161/200\n","354/354 [==============================] - 0s 290us/step - loss: 15.7441\n","Epoch 162/200\n","354/354 [==============================] - 0s 276us/step - loss: 14.3705\n","Epoch 163/200\n","354/354 [==============================] - 0s 293us/step - loss: 14.8494\n","Epoch 164/200\n","354/354 [==============================] - 0s 290us/step - loss: 16.6682\n","Epoch 165/200\n","354/354 [==============================] - 0s 285us/step - loss: 16.2002\n","Epoch 166/200\n","354/354 [==============================] - 0s 285us/step - loss: 14.9731\n","Epoch 167/200\n","354/354 [==============================] - 0s 287us/step - loss: 16.5514\n","Epoch 168/200\n","354/354 [==============================] - 0s 285us/step - loss: 14.2667\n","Epoch 169/200\n","354/354 [==============================] - 0s 296us/step - loss: 14.6268\n","Epoch 170/200\n","354/354 [==============================] - 0s 293us/step - loss: 15.3953\n","Epoch 171/200\n","354/354 [==============================] - 0s 279us/step - loss: 18.5149\n","Epoch 172/200\n","354/354 [==============================] - 0s 285us/step - loss: 16.1730\n","Epoch 173/200\n","354/354 [==============================] - 0s 282us/step - loss: 14.8511\n","Epoch 174/200\n","354/354 [==============================] - 0s 285us/step - loss: 14.5959\n","Epoch 175/200\n","354/354 [==============================] - 0s 287us/step - loss: 13.4741\n","Epoch 176/200\n","354/354 [==============================] - 0s 276us/step - loss: 14.6448\n","Epoch 177/200\n","354/354 [==============================] - 0s 287us/step - loss: 16.2899\n","Epoch 178/200\n","354/354 [==============================] - 0s 290us/step - loss: 15.2302\n","Epoch 179/200\n","354/354 [==============================] - 0s 276us/step - loss: 14.7434\n","Epoch 180/200\n","354/354 [==============================] - 0s 282us/step - loss: 14.4798\n","Epoch 181/200\n","354/354 [==============================] - 0s 282us/step - loss: 14.2832\n","Epoch 182/200\n","354/354 [==============================] - 0s 287us/step - loss: 13.9654\n","Epoch 183/200\n","354/354 [==============================] - 0s 293us/step - loss: 13.2887\n","Epoch 184/200\n","354/354 [==============================] - 0s 279us/step - loss: 13.0661\n","Epoch 185/200\n","354/354 [==============================] - 0s 349us/step - loss: 13.5760\n","Epoch 186/200\n","354/354 [==============================] - 0s 327us/step - loss: 13.4736\n","Epoch 187/200\n","354/354 [==============================] - 0s 304us/step - loss: 14.6688\n","Epoch 188/200\n","354/354 [==============================] - 0s 310us/step - loss: 13.7583\n","Epoch 189/200\n","354/354 [==============================] - 0s 282us/step - loss: 12.9293\n","Epoch 190/200\n"],"name":"stdout"},{"output_type":"stream","text":["354/354 [==============================] - 0s 293us/step - loss: 15.6787\n","Epoch 191/200\n","354/354 [==============================] - 0s 282us/step - loss: 14.8387\n","Epoch 192/200\n","354/354 [==============================] - 0s 282us/step - loss: 13.5870\n","Epoch 193/200\n","354/354 [==============================] - 0s 270us/step - loss: 20.0624\n","Epoch 194/200\n","354/354 [==============================] - 0s 285us/step - loss: 14.6527\n","Epoch 195/200\n","354/354 [==============================] - 0s 282us/step - loss: 13.3583\n","Epoch 196/200\n","354/354 [==============================] - 0s 273us/step - loss: 13.5116\n","Epoch 197/200\n","354/354 [==============================] - 0s 285us/step - loss: 14.2878\n","Epoch 198/200\n","354/354 [==============================] - 0s 290us/step - loss: 13.8896\n","Epoch 199/200\n","354/354 [==============================] - 0s 285us/step - loss: 12.9585\n","Epoch 200/200\n","354/354 [==============================] - 0s 285us/step - loss: 15.8158\n","실제가격: 22.600, 예상가격: 20.436382\n","실제가격: 50.000, 예상가격: 26.219849\n","실제가격: 23.000, 예상가격: 21.880560\n","실제가격: 8.300, 예상가격: 12.456472\n","실제가격: 21.200, 예상가격: 18.394142\n","실제가격: 19.900, 예상가격: 21.930637\n","실제가격: 20.600, 예상가격: 19.131042\n","실제가격: 18.700, 예상가격: 24.145924\n","실제가격: 16.100, 예상가격: 18.965050\n","실제가격: 18.600, 예상가격: 13.540152\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FOqQHCx-23S-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}